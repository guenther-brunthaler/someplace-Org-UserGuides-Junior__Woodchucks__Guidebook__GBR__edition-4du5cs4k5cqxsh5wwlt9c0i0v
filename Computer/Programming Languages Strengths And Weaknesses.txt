Programming Languages Strengths And Weaknesses

Quick Preview
Some programming languages did not seem interesting enough for me to actually evaluate them. Or I did not yet find the time to do so. This section contains what I have grasped so far.
* SPL (recursive acronym for "SPL Programming Language") is a dynamically typed language to be executed by a bytecode interpreter. There is a clear distinction between compiler, assembler and bytecode VM. Its speciality is that the state of the interpreter can be serialized to external storage or restored from there at any time. The language is intended as an extension language; especially for use in CGI scripts or as a procedural language in SQL databases (there is a plugin for sqlite). The language is based on C syntax, but without C's static typing. I was not impressed by the looks of the language, but maybe it is better than PHP and obviously can be used for similar purposes.
* "Shakespeare Programming Language" is a "fun language" where programs are intended to look like Shakespeare plays. Players represent variables, and there are syntactic means of constructing constants from bit values, storing them into variables and output the value of a variable either as a number or a character with that code. Goto and Labels exist, but it's all very strange. It seems SPL is implemented in Perl only and intended to be used as a Perl module for other Perl code to use.
* Cyclone is a C dialect which adds exceptions, buffer-overrun-protection, pointer-safety and run-time checks to the C language. It was primarily a research project and is no longer supported. The language authors recommend using Rust instead, which incorporates many of Cyclone's concepts.
* Ada is a heavily standardized general purpose programming language with a focus on security, reliability and readability of the source text for human readers.
* E is a programming language "designed from ground up with security in mind". Don't let be misled: This is actually a very JavaScript-like programming language with no special security features whatsoever. It just features some built-in functions for doing Public Key cryptography and managing capabilities, which are useful as basic building blocks in secure applications.
* Vala is what happens if one writes a program in C, adds more and more macros until the C code is barely noticable besides the macro invocations, and finally decides to take a step back and replace the macros by a precompiler which generates the C code of the macro expansions directly in a more flexible way. Finally one stops and realizes the result is no longer C. A new language has been created as a by-product.
* Ceylon is a language syntactically similar to C which compiles into byte code which can be executed using the standard JAVA VM. Its key feature is a fully static type system based on union and intersection types with powerful type inference capabilities. Compared to JAVA, Ceylon programs tend to be shorter and easier to read.
* Tcl ("Tool command language") is an interpreted programming language based on the simple concept "everything is a string". Is has a remarkably simple basic syntax and performs variable substitutions and word splitting much more predictable and reliable way than the UNIX shell. It seems this language must be very inefficient at run-time (but this is only partially true), yet it is also very elegant in its simplicity.
* Icon is a high-level programming language with extensive facilities for processing strings and structures.
* Rust is a programming language created by the Firefox developers. It is intended to be somewhat similar to C++ with the goal of making memory references safe. The concept of object ownership and how to transfer it is central to its design, and automatic destruction of heap as well as stack objects is tightly coupled to this. Rust is intended to be compiled like C or C++ code, and can also be used as a system programming language.
* SNOBOL is a string processing language where patterns are first-class data types. BNF-specifications can easily be converted into SNOBOL programs.
* Julia is a language for scientific computing which tries to close the gap between low-level "backend" languages like C and FORTRAN and "front-end" languages like Python. It is primarily a competitor to R and was inspired by Matlab. It uses LLVM and needs a lot of RAM to even start, and can provide an interactive REPL prompt like LISP systems. It is not an interpreter and compiles all code natively on the fly, but can also save it as compiled executables or be run as scripts.
* Wren is a scripting language conceptually very similar to Lua, but having a more JavaScript-like look. It is faster than Lua.
* JavaScript (officially called "ECMAScript" now) is the standard scripting language of all web browsers. For some time now, it has also been available for shell-like scripting via the node.js framework. The latter usage is the one which will be analyzed further in this document, because it is hard to tell the resource requirements of a script running within a web browser from that of the browser itself.
* Kotlin is a simplified version of the Scala language with a much faster compiler (one of the greatest problems of Scala). It provides most of Scala's power but is easier to use. The omitted parts are rather exotic and considered of little value in practical programs.
* Scala is pure object-oriented programming language (everything is an object) which favors a (non-pure) functional programming style. It generates code for the JAVA VM. That is, it is an alternative for JAVA targetting the same bytecode platform. It can also interact with JAVA objects, but not always vice versa. It is one of the most popular functional languages. It is also known for its very slow compiler.
* TypeScript is a superset of JavaScript which provides static typing, checked by a compiler which translates TypeScript code directly into JavaScript.
* Scheme is a minimalistic LISP dialect. It is very powerful, especially considering its small feature set. GNU guile is a well-known implementation which can be used as a POSIX-based scripting language.
Nim: A language with a similar indentation-based syntax as Python, but gets compiled into (supposedly) very efficient C source code.
* Zig tries to be a C-like language similar to Go but with manual memory allocation and better error handling.
* Crystal is a statically-typed language based on Ruby. However, it is not a replacement for Ruby and has quite a different execution model. Hopefully a much faster one. Because of its relationship with Ruby, it has also inherited some of Perl' features.

Identifiers
All prominent programming languages allow the programmer to assign user-defined names to variables, constants, functions and so on. Languages define different rules about the syntax of the identifiers, and sometimes identifiers are put into different logical classes which imply specific semantics of the identifier. In addition to this, most languages establish conventions how identifiers should be constructed for particular purposes, such as distinguishing named constants or type names from normal variables.
* C, C++ and many C-derived languages, identifiers consist of alphanumeric characters including underscore, not starting with a digit. In C and C++ there is a guaranteed minimum size of significant characters in an identifier name; longer identifiers may no longer be distinctive depending on the implementation. In most situation appropriate identifier sizes are supported, except for identifiers with external linkage in C where only 6 significant characters are guaranteed. However, few if any implementations actually implement those limitiations.
* C, by convention, uses names like this_is_a_name for everything except for constants (provided by the preprocessor), which look like THIS_IS_A_CONSTANT.
* In Go, the syntactic rules are the same as in C, but the letter case of the first character implies additional semantics: Upper case characters imply an externally visible name, other characters imply module-local visibility only.
* Ada does not distinguish between upper- and lower-case letters; neither in identifiers nor in language keywords.
* Identifiers in Tcl can really be arbitrary strings, but in order to avoid having to quote them in normal use they have to be made up of plain ASCII alphanumeric characters plus underscore.
* Julia allows most UNICODE characters in Names, but interprets characters from the mathematical symbol plane as infix operators by default. Many built-in operators also use a mathematical UNICODE character as an alternative to the primary symbol made of ASCII characters. In the case of the integer division operator, there is no ASCII alternative and so the UNICODE symbol must be used. (The functionality of this operator is also available as an ASCII-named function though, so this is not really a problem.) In Julia's interactive command line interface, mathematical UNICODE symbols can be input via tab-completion by typing a backslash followed by the symbols ASCII name followed by the tabulation key. There are hundreds of names available for expansion via this method, allowing them all to be input using a simple standard ASCII keyboard only.
* JAVA identifiers can contain all UNICODE letters and digits, plus underscore and dollar sign. However, normal identifiers must start with a UNICODE letter. There is no length restriction.
* JAVA, by convention, uses THIS_IS_A_CONSTANT for constants, TypeLikeThisOne for type names and namesLikeThisOne for everything else. The dollar-sign shall never be used and is reserved for machine-generated identifiers.
* Wren allows the same identifiers as C. Leading underscores have special meaning when used inside classes: One leading underscore means an instance variable, two leading underscores mean a class variable shared by all instances. In addition, letter case can affect the name lookup for method dispatch in certain situations.
* FORTH identifiers can have a size from 1 to 31 characters and are made of printable ASCII characters (i. e. neither whitespace nor control characters). In particular, they are not restricted to alphanumerics and often include characters forbidden in identifiers of most other languages.
* Kotlin uses camelCase for idenfifiers by convention, and even some keywords (such as "downTo") use camelCase.
* Scala used camelCase for names of variables and methods, and PascalCase for type names.
* Nim's identifiers are case-insensitive except for the first letter. In addition, underscore characters are allowed in the names but will be ignored. The argumentation about these rather odd rules is that every programmer can use their preferred style of idenfifiers, because they will all have the same effect. That is, most variants of camelCase and snake_case can be used interchangeably, so hopefully everyone will be happy. Keywords cannot be used directly as identifiers. However, by enclosing a string within backquotes, nearly any string can be used as an identifier. This is called "stroping".

Variables and constants
Most programming languages provide variables. Sometimes they must be initialized immediately during definition, in other languages they only need to be initialized before reading them the first time, many languages provide a default value, and the remaining languages just don't care and return unspecific values in this case. Sometimes, variables can be read-only: They can only be written once. This can either be done via an initializer expression as part of the variable definition, or later. In addition to that, many language also support real constants, which are calculated at compile time rather than at runtime. Additional protection is usually possible for classes, objects and structure member fields. These may affect visibility as well as read/write permissions and are frequently based on class membership of the invoking object.
*In Nim all variables must be initialized when they are defined. It is however possible to assign the same value to all of them. Strangely enough, this is just syntactic sugar for writing the same number of separate definitions. And the dangerous thing about is that any side-effects from evaluating the initializer value will also happen for every of the assignment. To be clear, if the initializer expression contains a function call, the same function will be called for every assignment of the same value to one of the variables which share the same initialization expression. This is just crazy! For once it wastes time calling the function multiple times. And then it is dangerous because the function may have side effects. Just think of calling a random number function: Every variable would then be assigned a different random number. Nim has "var" for defining normal read/write variables. It also has "const" which is similar in usage but defines a compile-time constant. Finally, there is "let" which defines run-time a constant. This is like a normal variable, except that the only place where a value can be assign to it is its definition initializer. Multiple variables can be declared in a single "var" statement in the same line, or any number of lines with declarations may follow by indenting those more than the "var" line.

Comments
Comments are an important part of every programming language. Even though they normally have no function to the program itself, the provide important hints to a human reader about how to use the code, or explaining how it works. There are basically 2 types of comments, single-line and block comments. Block comments may span multiple lines and are not just a repetition of single-line comments.  Some languages allow multi-lines comments to be nested, others do not. Single-line comments normally consist of a starting token and last until the end of the source line. Block comments have usually starting and ending tokens which delimit the range of the comment block.
* C and C++ support block comments with "/*" and "/*". They cannot nest. In addition, the C preprocessor can be used to exclude a block of (whole) lines, and this type of (de-facto-) "comment" *can* nest.
* All C++ versions, but only C99 and newer versions of C, also support single-line comments starting with "//".
* Rust only supports single-line comments. There are different types of those. "//" are normal comments, "///" are documentation comments using markdown syntax, and "//!" are a special variant of documentation comments which are put *inside* the entity to be documented rather than before it. This is required to document the source file as a whole, for instance, rather than some part of it.
* JAVA uses /* */ like C and has nothing else.
* Wren uses the same comments as C++ except that C89-style block comments can be nested. In addition, a standard UNIX shebang line seems also to be ignored if present. However, this might actually be a feature of the existing interpreter implementation rather than the language itself.
* Julia has "/* ... */" block-comments and "//" single line comments like in C99, except that the first ones can be nested.
* Scala uses the same comments as Julia.
* Nim has "#" line-comments and also supports block comments with "#[ ... ]#" which may be nested. In addition, it has a "discard" statement which accepts and ignores any arguments. When a triple-quoted string is passed as an argument to this statement, the effect is pretty much the same as a block comment.
* Scheme up to version R5RS only has line comments, which is everything following a semicolon in that same line. There is a convention that a single semicolon shall be used to add comments after other tokens in the same line. Two semicolons are used for indented comments on their own line. And three semicolons are used for full-line comments which are not indented and start at the beginning of the line. R6RS adds block comments "#|" and "|#" which can be nested, and "#;" comments which comment out just the following syntactic form/expression.

Memory Allocation
Many programming languages implement the concept of garbage collection for automatic memory management. Other paradigms are automatic reference counting (ARC) or explicit allocation (i. e. malloc, free, new, delete). GC is generally the easiest to implement, but represents a space-time-tradeoff and requires considerably more memory than explicit management. ARC avoids the memory overhead of GC, but can be considerably slower when the reference count operations are designed to be used by multiple concurrent threads.
Another question is whether allocated memory can be relocated or needs to remain at its initial allocation address until it is freed. The ability to be relocated allows for automatic memory compaction, while static addresses can lead to internal fragmentation of the heap over time which increases the required amount of memory. Many GC-implementations support memory compaction, while ARC-based or explicit allocation rarely provides support for it. Automatic memory compaction is usually only possible if a programming language is restricted to references and does not support pointers or pointer arithmetic on address values.
* Tcl (at least up to version 8.5) uses ARC.
* Vala uses ARC. This is one of the few advantages of Vala over JAVA, if you dont' like GC.
* Python uses GC. Needs to be verified: Maybe ARC is also involved?
* Lua uses GC without support for memory compaction.
* C and C++ manage memory explicitly, but this may be hidden in object destructors or cleanup functions which have been registered in some resource allocation framework.
* Icon uses GC.
* Perl uses a combination of ARC (for normal operation) and GC (a mark/sweep GC for eventual cleanup on exit). This hypothesis needs to be verified!
* JAVA uses GC and provides memory compaction. This needs to be verified.
* JAVA allocates normal variables of native built-in types (such as "int" or "double") on the stack, and everything else on the GC-controlled heap. This even applies for very short non-native types like "complex".
* JavaScript uses GC. It is unclear whether this also supports memory compaction.
* Rust supports creating objects on the heap or on the stack, like C. Rust does not use GC, but rather tracks object ownership in order to know when objects can be destroyed, including heap objects. To this end, object ownership is distinguished from mere object references. For instance, if an object variable goes out of scope which owns the object, the compiler automatically deallocates the object at this point. In Rust, calls to malloc() and free() are generated automatically by the compiler, rather than by the programmer.
* SNOBOL uses GC.
* Go uses a traditional GC, although it has been heavily optimized to only incur small delays during program execution. It has pointers, but no pointer arithmetic. However, there is a special "unsafe.Pointer" type in the standard library which does support pointer arithmetic and allows to implement low-level memory management facilities. However, "normal code" should never use that.
* Wren uses garbage collection like Lua, but provides less control over it. In particular, there is no support for weak tables, GC callbacks or standardized GC parameter tuning at run-time. However, at least 3 basic GC parameters can be set up before firing up the VM from within am embedding application.
* As of version 1.4.2, Nim provides no less then seven different kinds of dynamic memory management systems the developer can choose from. This includes the dreaded BoehmGC, but also a simple mark & sweep deallocator, automatic reference counting and even "None" which simply does not free allocated memory any more. (Perhaps this allows manual memory management like in C?) In any case, Nim provides more options and alternatives for dynamic memory management than any other language I know of.
* Crystal is a GC-basesd language. As of version 1.7, Crystal unfortunately uses BoehGC, possible the most inefficient GC in the world.
* Swift uses ARC. Reference-modifiers "weak" and "unowned" allow (if properly applied) freeing objects even in the presence of cyclic references.
* Dart uses GC.
* TypeScript uses GC.

Co-Routines
Co-Routines allow to implement producer-consumer-scenarios, where both consumers and procucers of some resource consider each other as subroutines to be called; either consuming or providing the next resource item. But in fact none of them are subroutines, they are co-routines instead: A "procedure call" just transfers control between two otherwise independent flows of control.
* SIMULA was the first widely-spread language to feature coroutine support. Until recently, not many languages have also provided such support.
* Python has "generators". Those are special functions which can yield values during their execution, and consumers using those functions will get the next result back from the function call each time the producers yield.
* Ruby also has a yield statement with similar capabilities. But Ruby does not support true co-routines, as using "yield" is not symmetrical.
* Perl5 does not support coroutines in any way. But upcoming Perl6 is said to have such support. The coroutine mechanism's syntax will look similar to pipe usage. However, fork() can be used in Perl5 to emulate the behaviour of co-routines to some extent.
* Although not a language, the Windows operating systems starting with NT4 provides "Fibers" to all applications running on those operating systems. Fibers are something like a lightweight thread, but are not scheduled on a timeslice or I/O-blocking basis. Instead, the different fibers hand over control flow to each other by operating system primitives similar to a "yield", which in fact turns fibers into co-routines.
* Also not being a language, the GNU glibc provides a ucontext_t data type with supporting functions, which allows to emulate the functionality of Win32 fibers to some degree. However, this solution is incomplete and suffers from the same drawbacks as the setjmp/longjmp family of functions. Outdated versions of POSIX provided a similar function, but it has been withdrawn in newer versions because it was felt POSIX threads provide a similar but far more powerful functionality.
* Erlang does not have actual co-routines, but provides very lightweight pseudo-processes which can be used for much about the same purposes. Erlang processes will be scheduled by the language runtime itself and will thus not have OS context switching overhead, except for SMP and distributed scenarios where more than a single processor is actually used. Erlang's send ("!") and "receive" operators can then be used to "invoke" processes in a co-routine-like fashion, or in order to return results back to the "caller".
* Ada does not have special co-routine support, but its "tasks" can be used for a similar effect when combined with synchronization and serialization of parallel processing. Most likely, this will not be a very efficient substitute for co-routines.
* Tcl 8.6 is said to provide real co-routines. In addition, even older versions of Tcl have an "interp" commmand which allows creating independent interpreter instances. This could also be mis-used to emulate rather expensive co-routines, yet it is still less expensive than forking another Tcl process which is also supported by the language.
* Lua has full support for co-routines. Even "C"-code calling Lua code may support co-routines, if the developer makes use of the continuation-aware Lua API functions for calling a Lua function. However, Lua co-routines are asymmetric: There is a difference between caller and callee. It is possible, however, to implement traditional co-routines with this, by creating a helper co-routine which resumes all co-routines, and those yield to the helper telling it what co-routine to resume next. Unfortunately, Lua switches co-routines by calling the longjmp() function of the standard "C" library, which is a rather expensive function to call for CPUs with many hardware registers, as most CPU registers need to be restored to previously saved values by longjmp(). Lua also uses longjmp() for raising exceptions, and thus resuming a co-routine and raising an exception have similar overhead. On the other hand, this will only be a problem if a co-routine does nearly nothing and then immediately yields. If a co-routine does serious work, such as handling the next service request of a network service, the longjmp() overhead will be irrelevant in comparison.
* Julia has co-routines named "Tasks". Control is passed between co-routines by calling the special functions produce() and consume(). For more direct control, yieldto() is also available. It is unclear how co-routines are implemented and how performant or expensive they are in terms of system resources.
* Go does not have co-routines, but rather concurrent threads named "goroutines". Using proper synchronization, goroutine execution can be serialized as if they were co-routines and perform the same tasks, albeit with much more overhead than native co-routines.
* Wren provides "fibers" which are quite similar to Lua's co-routines. They are in fact co-operatively scheduled "green threads" without any kind of pre-emption implemented by the interpreter and do not use OS threads. Fibers are scheduled directly by the interpreter and do not use expensive setjmp/longjmp as in Lua. As a consequence, yielding is much faster in Wren than in Lua.

Destructors
* C++ has synchronous destructors. For instance, it is possible to implement a "file" object, and the destructor will close the associated file as soon as the file object is destroyed, and this destruction will take place as soon as the object variable leaves its scope. Destruction of shared objects will usually be controlled via reference-counting schemes instead.
* Perl has pretty much the same destructor capabilities as C++, but the reference counting for shared objects is handled by Perl automatically. Like in C++, Perl's destructors are also synchronous. Destructors are member functions with the special function name "DESTROY". In order to correctly clean up circularly-linked structures where reference-counting alone will fail, a mark-and-sweep garbage collector is run before exiting out of a Perl program. This mark-and-sweep is not done, however, during normal operation when the application keeps executing afterwards. Which means circularly linked data structures must still be cleaned up manually by the application, or they will waste memory until the application exists.
* In practice, Python supports destructors nearly the same way as Perl does, except that the special member function __del__() is used as a destructor. But unfortunately, the language definition does not require that destructors are called as soon as possible, although in the most popular implementation they are. This means portable Python code cannot rely on using destructors for effective automatic runtime resource management, although it will work very well in the current primary implementation.
* JAVA has asynchronous destructors. This means they will be run at some point in the future, usually as a consequence of garbage collection. But there is no easy way to control *when* the destructor will actually be run. This renders JAVA destructors mostly useless for things like a "file" object which automatically closes the file on destruction. Instead, JAVA developers are forced to use the "finally"-construct for such purposes, essentially shifting the burden of destructor processing from the language to the application developer. I consider this to be one the largest shortcomings of JAVA.
* D provides a mixture of the C++ and JAVA approaches: While it uses JAVA-like destruction and garbage collection by default, it is also possible to define C++ like destructors for objects which need synchronous destructors (most won't) using the "auto" keyword. Unfortunately, "auto" is not part of the declaration of a class, but rather part of the variable declarations of objects made from that class. And even worse, the compiler does not enforce that "auto" is either used or not used when defining a variable of a particular class. In my opionion, this is an essential flaw in the language design, and renders the use of C++ like destructors highly dangerous and error prone. Actually, I considered this flaw as the primary reason why *not* to use D instead of C++.
* FORTH has no native objects (although they are available as extension packages implemented in FORTH) and therefore no destructors. But its simple exception processing features might be used to implement limited control-flow-triggered cleanup code in the same way JAVA allows it.
* Erlang does not have objects, and therefore no destructors. However, processes can "linked" to other processes and act upon the termination of those other processes. This can also be used for freeing resources and committing or rolling back transactions. Erlang also has try/catch capabilities which can be used to invoke destructor-like code, but those features are not commonly used in Erlang programs.
* Rust has destructors, but it seems they are bound to values instead of classes. They get triggered as soon as the value is destroyed, such as by going out of scope. The drop() method is used as the destructor when implementing the std::ops:Drop trait for a type.
* Ada classes can define destructors by declaring a Finalize() procedure.
* Tcl has no native OO-capabilities, but since Tcl 8.6 a module Tcl::OO is available which provides the most important object-oriented features. Destructors are one of them.
* Wren does not have destructors.
* Nim has destructors.

Typing disciplince
There are dynamically and statically typed languages. In dynamically typed languages, the type of an expression or variable can change. In statically typed languages, the type cannot be changed once the code has been compiled, even though type polymorphism may be supported. Statically typed languages always use some sort of declarative typing, although type inference may be provided to avoid writing declarations in many situations. Dynamically typed languages may also use declarative typing, but many use "duck typing" which means variables do not have a type at all; the values stored in the variables have a type. Typing can be weak which means that type conversions will be performed automatically on demand, or it can be strong which means necessary type conversions must be performed explicitly. There is structural and named typing. Structurally typing is like duck-typing, only applied to types instead of values. It means that type names are just alias declarations, such as "typedef" in C. This allows differently named types to be used in the same situations, as long as they refer to the same underlying type structure. Named types, on the other hand, may refer to an identical underlying type structure but will still be considered different types because the name is different. IMO, weak typing, duck typing and structural typing are dangerous misconceptions which lead to error-prone code.
* Rust is a statically typed language, and all argument types of a function must be explicitly declared. Within a function body, however, type inheritance may be used if desired. It uses structural typing only.
* Ada is one the most statically typed languages in existence. Most things can be checked by the compiler at build time, and the remainder will be checked by automatically generated run-time checks. Ada heavily relies on type names, rather than structural equivalence. An integer which is declared to be of type, say, "age" can never be added to another integer declared to be of type "temperature" by accident. Type conversions are possible but not performed automatically. For situations where variables of different types *shall* be allowed to be used in the same arithmetic expressions, subtypes can be used in order to eliminate explicit type conversions.
* Ceylon uses powerful static typing. Types can be combined using union type (multiple types as a new one) and intersection type (the common traits of multiple types as a new type) operators. "Nullable" types are available and are internally contructed using union types. Special variants of common control statements allow to distinguish types and narrow down the allowed range of types within a particuar control flow branch. Usage of those statements is also enforced in order to create type-safe code without a chance for runtime exceptions due to runtime type mismatches. Type assertions can also be specified using assert() and will be examined by the type checker. Unfortunately, there is no way to distinguish such assertions from "normal" assertions, and all assert() statements generate runtime overhead which cannot be disabled selectively (or at all).
* In Tcl, "everything is a string" (EIAS). This is not completely true with respect to variables, however, which differentiate between simple scalar variables (which contain really strings) and "arrays" (hashes). Also, behind the scenes, Tcl can store a variable as a string as well as using an internal representation. Both representations can be valid at the same time, or just one. In the latter case, the missing representation will be re-calculated from the other one as soon as it is needed. Anyway, EIAS means that Tcl is weakly typed, always converting internall-required argument types of commands from strings automatically if needed.
* Icon has weak typing. Values are automatically converted beween different type representations depending on what is expected.
* Rust is statically typed with optional type annotations and type inference.
* Julia is dynamically typed - only values have types, and names just bind to them. However, type annotations can be attached to names, restricting the types they accept.
* Go uses simple static typing much like C. However, other than in C, typing is mostly nominal: If a structural type is assigned a name, then other types with the same or a matching structure layout are *not* considered to be of the same type. Aside from simple automatic conversions for built-in numeric types, no automatic type conversions are applied and most type conversions must therefore be explicitly invoked. Also, Go provides optional type inference via the ":="-operator. Unfortunately, Go's typing discipline for interfaces is structural rather than nominal. In other words, they use "Duck-Typing" for determining which methods apply to which types.
* Wren has dynamic typing, much like Lua. Only values have type, variables just bind to values of any type.
* Scala is statically typed, but provides very powerful type inference system which allows to skip explicitly stating type names in many cases.
* Kotlin is like Scala, but has the distinct concept that type cases are inferred from type tests. For instance, if an "if"-condition tests the type of a variable, the variable will automatically be considered casted to this type within the body of this "if"-branch.
* Nim is statically typed. It provides generics and also powerful template-like macro capabilities. There is limited support for type inference which only works for variable definitions.
* Crystal is statically typed. However, types are derived from constants or initializers where possible, so type declarations can often be omitted.

Multiple Inheritance.
* C++ fully supports multiple inheritance. It is even possible to inherit from multiple base classes containing conflicting symbols without actually generating name clashes. Despite the fact they are sharing the same name, such symbols are not merged into a single symbol by inheritance, but rather stay separate entities. Ambiguities resulting from this are solved by qualifying ambiguous symbols with the name of the superclass those symbols have been inherited from.
* JAVA does not have multiple inheritance, but supports multiple "interfaces" per class instead. Interfaces are much like pure virtual classes in C++.
* Ruby does not provide multiple inheritance, but supports "mixins". Mixins are much like a macro, adding the same attributes and methods to each class definition choosing to include a mixin.
* Perl supports a restricted form of multiple inheritance, where it is left to the developer to avoid name clashes when the attributes of multiple classes get merged together into a single object. From an implementation's point of view, Perl implements multiple inheritance by letting each class specify a list of superclasses which will all be searched recursively if a referenced attribute is not found in the derived class itself. Perl therefore only supports C++'s equivalent of "virtual base classes", but not normal (non-virtual) base classes.
* Python supports multiple inheritance in the same way as Perl.
* FORTH has no objects and therefore no inheritance. However, it supports the definition of multiple dictionaries for storing words, which will be searched in a specific order when looking up words.
* Erlang has neither classes nor objects, and therefore also no inheritance. However, particular function names can be chosen which a module must export in order to provide some defined functionality. This can be used to emulate interface declarations to some extent.
* Objective-C only supports single inheritance but allows to emulate most effects of multiple inheritance by providing "protocols" which are similar to JAVA's interfaces. In addition, Objective-C supports "categories", which allow to add methods to or override existing methods of an existing class without modifying or even recompiling the class itself. Instead, a modified copy of it will be created, from which objects can be instantiated which are compatible with objects created from the original class, even though they can display modified or extended behaviour. This allows for "monkey-patching" a class at runtime without a need to make any changes to the original class implementation. However, this feature is regarded to be error-prone and is not a generally recommended technique.
* Go does not have classes, and therefore neither single nor multiple inheritance. However, it allows to bind arbitrary functions to structs by declaring a function as bound to some struct type as part of the function declaration. This allows the function to be called upon the struct, passing the struct as a named variable to the method. Actually, methods can be defined for every user-defined data type, not just for structs. A group of function signatures can be defined to represent an interface. Variables of interface types can be assigned objects implementing all (and possibly additional) functions of that interface, and the methods can be called upon those interface-restricted object variables. Instead of multiple inheritance, the same object variable can therefore be assigned to different interface variables which ensures the object implements all required methods. Interfaces can include other interfaces. This does not created a nested relationship but rather leads to a merging of the individual methods of the contributing interfaces into the new interface.
* Ada only features single inheritance directly via "tagged records", but more recent Ada versions have added support for JAVA-like interfaces which can reproduce the most important features of multiple inheritance.
* Tcl has no language-level support for objects, and thus does not support any inheritance. Multiple object-oriented extensions for the language are available as add-ons, though.
* Tcl's standard object extension Tcl:OO, which is shipped along with Tcl since version 8.6, supposedly supports multiple inheritance. But even if it did not, alternative OO-frameworks for Tcl exist which do. Therefore, Tcl as a framework supports multiple inheritance, just not as part of the core language which completely lacks OO support.
* Wren does not seem to support multiple inheritance.

Error handling.
Some languages uses exceptions for error handling, others use return values to indicate errors to the caller.
* C does not have any support for exception at all, unless the standard library's functions for raising and handling signals are interpreted as such. But even then portable programs cannot use signals for exception handling, because there is not standardized set of signals available in all implementations. Error handling in the C standard runtime library is based on return values and a global variable or macro named "errno". Special return values are reserved to indicate an error, and "errno" provides the actual error code. Custom code is free to used alternative ways of error handling, though. The setjmp()/longjmp() functions of the standard runtime library allow to implement non-local jumps, which can be used to implement an exception-like mechanism for error handling.
* C++ has full basic exception support, but does not allow double-exceptions, i. e. it is not allowed to throw an exception while stack unwinding ("cleaning up") during exception handling is in progress. That is, destructors must not throw an exception if they are called during exception processing. Using the std::uncaught_exception() builtin function it is still possible to write exception-safe destructors. The downside is that there is no way to pass sub-exception objects caught during exception handling to the higher-level exception handler without using global or thread-local variables.
* JAVA does have full exception handling support. The "finally" keyword allows to execute instructions when a scope is left either by normal control flow or by means of exception processing.
* Objective-C's exception handling is much like JAVA's.
* Python's exception handling is much like JAVA's.
* Ruby's exception handling is much like JAVA's.
* Perl supports exception handling via die/croak/eval, but it looks somewhat crude and not very elegant. Most of the time, Perl uses special return values in order to indicate failures, very similar to C. Information about the error can be found in global variables "$?" (child processes), "$!" (OS errors) or "$@" (from eval).
* FORTH has a "catch" word which takes a code address and executes that code. If an unhandled exception is encountered during that execution, "catch" completes with the current argument stack intact and adds the exception code to to TOS. If there is no exception, the stack is cleaned up and an exception code of zero is pushed at the TOS. Execption codes are integers. Some words, especially I/O related ones, return an exception code or 0 as one of their results. The "throw" word can be used to check those codes; it consumes a code of zero and throws an exception for any other value. "catch"/"throw" therefore provide simple exception handling and can also be used to implement some sort of "finally" semantics.
* Erlang really likes exceptions. It generates them with pleasure, and the language provides lots of occasions for raising runtime errors. The most errors are type mismatch errors (such as passing an integer where a float is expected or vice versa) as the language uses "Duck Typing": The compiler normally neither checks not enforces correct type usage, but the runtime will raise exceptions as soon as a type mismatch is detected for a particular built-in function or operator. In contrary to JAVA or C++, however, try/catch is typically *not* used in Erlang to catch exceptions. Instead, the runtime will terminate the process which raised the exception, and other processes in the application are assumed to monitor such events and act accordingly.
* Rust has destructors, but it seems it does not have any concept of exceptions. The only thing similar to an exception is to set a "task" (green thread) to the "failed" state, in which case it unwinds its stack, calls all destructors, and then terminates. However, there is no way to "catch" and stop such an unwinding process. It may be temporarily suspended, but in the end the task will be destroyed. By convention, error handling in Rust distinguishes between recoverable and unrecoverable errors. Unrecoverable errors call the panic!() macro which unwinds the stack or aborts a task (this can be chosen in the project configuration). panic!() provides the user with an error message and an optional stack backtrace and also terminates the faulting task. Recoverable errors are handled by returning a tagged union named 'Result<T, E>' which can contain either a normal result of generic type T or an error object of generic type E. The caller can either check the kind of returned object manually, or call a helper function such as unwrap() or expect() in order to extract the T value from the result (or raise a panic!() if an error object has been returned). This is an unfortunate design decision which means most production-level Rust source code is cluttered with unwrap() or expect() calls for error handling.
* Go 1.x did not have exceptions. Errors were always represented by error objects, which is any object type implementing an Error() method returning an error message string. Go functions which can fail can return such an error object, and the caller may check for it. On the other hand, this is exactly the same type of error handling C "provides", except for that special builtin-type which suggestes all errors will be represented uniformly. In contrary to C/C++, however, Go supports multiple return values from functions. The Go convention for error handling seems to be that all functions which can produce errors return an additional result containing an error object if an error occurred. This additional result may then be checked by the caller, but nothing forces the caller to actually do this. Later versions of Go have "panic" which makes the current function abort and return to its caller, where it raises the same panic again until finally the whole program aborts. However, if "recover()" is called in the "defer"-cleanup statement of one of the outer calling functions, then this will catch the panic, resuming normal processing. The basic difference to try/catch is that panic/recover only provides function call granularity. The convention for external library interface APIs is still to use error objects as return values rather than panics, though.
* Zig "borrows" the "defer" concept from Go for scheduling code to be executed once the function returns and can, for instance, be used to release resources allocated locally by the function. To this, Zig adds "errdefer" which only runs the scheduled code in case of an error, ignoring the code if everything went fine. This allows to deallocate partially-constructed objects in case of an error, where the ownership would have been be passed back to the caller if there had not been an error. Zig does not have real exceptions, though.
* Ada has simple named exceptions. Exceptions are not inherited but individual entities of type "exception". The "raise" procedure is used to throw exceptions, and exception handler can catch them. Newer Ada versions also allow to specify an error message along with the exception name when raising an exception.
* Tcl features a "catch" command for capturing exceptions. The return value of the command indicates whether an exception has occurred. It is also possible to specify a variable for obtaining the normal return value of the guarded code or an error message. Besides error exceptions, three "normal" exception indicator codes can also be returned by "catch", which result from the execution of one of the following Tcl commands: "break", "continue" and "return". In addition to those predefined return values of "catch", arbitrary custom exception codes can be also be generated. The "catch" command can optionally set up an associative array containing additional information about the exception. This includes nesting level, stack backtrace and other information. There are also global legacy variables which shadow the result of the last "catch" invocation, but they need not be used.
* Wren does not have exceptions per se, but its fibers can be used for emulating ones. The code to be run in the "try"-section must be run as a new fiber while the "catch"-code remains in the outside/original fiber. Runtime errors are thrown be setting a fiber-specific error message and then terminating the fiber. Normally this would propagate and terminate the "calling" fiber as well, but if the failing fiber has been invoked with the "try" fiber method, it will just return the error message which the "calling" fiber can then check for and act accordingly.
* Nim provides exception handling via "try"/"except" and has both a JAVA-like "finally" as well as a much cleaner "defer" which schedules code to be executed at the time the current "try"-block is left. In addition, Nim provides synchronous destructors. Contrary to JAVA, Nim's destructors are not triggered  at some non-deterministic time by garbage collection but rather when a local variable goes out of scope.
* Scheme has powerful exception-handling primitives, but unfortunately exception handling support was not standardized at all before R6RS. Most R5RS implementations did provide exception handling support, but it was not portable. Since R6RS, portable exception handling is possible for many standard library functions via "conditions" which are class-like types for error identification.
* Dart has exceptions.
* Swift returns error objects. This is somewhat hidden behind syntactic sugar or helper functions which makes it superficially look like exception handling. But these are not real exceptions, and error handling remains tiresome and error-prone.

RAII.
Resource Acquisition Is Initialization. Supporting this approach means safe creation and automatic cleaning up of resources. Objects which support RAII are always in a defined state, and track any resources allocated to them which are automatically freed as soon as the object is destroyed. Any resource allocation and deallocation related to some object works in an atomic fashion from the view of the object clients. The most important goal of RAII is avoiding resource leaks or leaving objects to be partially-initialized in case of exceptions.
* C has no direct support for RAII. A framework for RAII can be implemented without direct language support though, using the standard library's facilities for non-local jumps. Exception-, destructor- and "finally"-like functionality can be implemented with those non-local jumps, although global or thread-local variables are typically required in implementations for tracking the RAII state.
* C++ has full support for RAII, but unfortunately does not enforce it in any way. The most important components for RAII in C++ are its constructors, synchronous destructors and exception handling. The biggest problem with RAII in C++ are its old-styled runtime libraries which rarely if ever use exception handling to indicate runtime errors. Another problem is its braindead "iostreams" concept which links global state such as formatting information to streams which is very problematic in the context of exception handling. Using RAII in C++ therefore usually requires relying on custom classes or wrapping standard library features into some RAII-friendy wrapper.
* Perl, having mostly equivalent capabilities like C++ in terms of its synchronous destructors or exception handling also has the same problems with RAII: While Perl provides anything necessary for implementing RAII, the builtin functions and its standard library usually do not make any use of it. This requires writing custom wrappers and wrapper classes for exploiting RAII. Newer Perl versions provide the "autodie" module as a mitigation, but unfortunately this module only wraps a few dozen build-in functions for automatic exception throwing in case of errors. It does not even care to wrap the most important output function, "print".
* JAVA has exceptions and garbage collection, but it's lack of synchronous destructors makes implementation of side-effects aware RAII impossible. At least via destructors. For instance, while garbage collection ensures any deallocated memory will be reclaimed sooner or later, it is impossible to create some "File" object which automatically closes the file when the object goes out of scope. Only the "finally" keyword can be used to do such cleanup, but this is error-prone because it is possible to simply forget putting a "close"-method invocation into a finally handler without the compiler complaining. Also, whether cleaning up is necessary for an object when its scope is left and how this clean-up is to be done clearly is an object-specific internal, and not something to be done by the client of an object, whether in a finally handler or elsewhere.
* Ruby has the same problems like JAVA, but it allows a trick to be used by mis-using generators in order to provide synchronous pseudo-destructors. That trick consists of providing constructor-like functions which also contain destructor logic and "yield" before running the destructor part. The code which is run at the "yield" is code passed to the constructor-like function as a code block. The constructor-like function thus initializes the object, yields to run the code block, and then cleans up the object. If the "yield" is protected by a exception/finally handler, the net effect of the whole is to behave as a synchronous destructor supported by exception handling - and thus all required for RAII. Unfortunately, this also restricts the lifetime of the RAII-controlled object to the code block which is passed to the constructor. Another problem of this approach is that the object will already have been destroyed when the first client-exception handler gets a chance to catch the exception. Also, multiple RAII-controlled objects require as many code block nesting levels. This can become very unhandy in situations where many RAII-controlled objects are required to work with.
* "D", although propagating RAII, is actually rather inappropriate in using it due to its crazy implementation of the "auto" keyword. (Later revisions of the language seem to use the "scope" keyword for that instead.) Theoretically, "auto" would be all you need in order to get synchronous destructors, the corner-stones of RAII. But by some fundamental design flaw, "auto" is not part of the class definition, but rather part of the variable definition for class objects. Forget to write "auto" at some variable definition, and your object will *not* get a synchronous destructor, but behave like a JAVA object instead which is incapable of doing side-effects aware RAII. All which "D" needed to change is allowing to include "auto" with the class definition rather than the variable definition. But it doesn't. And so it's practically unusable with RAII.
* Erlang automatically provides basic RAII, because it is a single-assignment language: As soon as a variable is bound to a value, this will continue be the variable's value for the lifetime of the current scope. Before that assignment, a variable is "unbound" and will automatically generate an exception if used within an expression (except for a match expression, which will then bind the variable instead). In any case, the variable is always in a defined state. What Erlang does *not* provide, however, is automatic transactional cleanup via destructors, because there are neither objects nor destructors. This is not a problem for allocated memory as Erlang is a garbage-collected language (accounting to about 5 % of the total running time of an application). However, it might be a problem for other database-style transactions. This must then be emulated using supervising processes which act on the termination of monitored processes.
* Python has no official support for synchronous destructors, although the current primary "C"-based implementation actually has them. However, there also exist other implementations which do not support synchronous destructors. Thus they cannot be assumed to work. Instead, Python's destructors must be assumed to have similar semantics like JAVA's destructors, which makes them inappropriate for RAII. However, modern versions of Python added the "with" Statement together with related standard runtime support. Using this statement, compliant objects can be written which define a method to be used as destructor when the scope of "with" is left. This is a drastic improvement compared to the "finally"-metaphor used before this, because the user of an object does no longer need to know the details of destruction as long as he uses "with". While this new approach is quite better than the old one, it is still no replacement for C++'s synchronous destructors: The "with"-metaphor only works in situations where it is possible to limit the lifetime of objects to stack frames. This can be problematic for caches containing dynamically allocated objects which are to be freed at a time mostly independent of the scopes of the functions which created those objects. The "with"-metaphor is also inappropriate related to inheritance, because it binds resources for deallocation of each subobject of a composite object, rather than a single destructor function which destroys the object along with all of its subobjects as a single operation and no need for additional external pointers for the subobjects.
* Rust has destructors but as it is missing exceptions, the only way to do RAII seems to be using green threads ("tasks") as "bodies" for catching failures. When a task is put into the "failing" state, it will clean up its resources by unwinding its stack and terminate, which can be waited for by another task. However, this sounds awfully inefficient. Whether this is actually the case remains to be examined.
* Ada has exceptions, exception handlers and destructors. So there seems to be everything there needed to implement RAII.
* Tcl does not natively have objects or destructors, although object-oriented language extensions do exist. RAII can only be achieved using the "with-object-do"-pattern in the core language. Transactions have to be implemented as normal functions which manage the resources affected by the transaction, and the code to be run protected is accepted as a callback and will be run under the control of an exception handler which rolls back the transaction in case of an error.
* Go has neither destructors nor try/finally, but it features the "defer" keyword which allows to schedule a function call (its arguments are evaluated immediately, though) for the time when the current function returns. "defer"can also be used in loops, which then stack the scheduled invocations and will execute them in reverse order later.
* There is no direct support for RAII in wren, as it has neither destructors nor GC "destroy" callbacks nor "finally"-functionality nor exceptions. RAII semantics like "stack unwinding" or "destructors" must therefore be implemented by the program itself, such as by providing some sort of resource registry mechanism shared by the "try" and "catch" parts of exception handling. And the exception handling itself needs to be emulated by using fibers.
* Nim seems to have everything required for RAII.

Lambda Objects and closures
Lambda Objects are unnamed, executable units typically rather temporary in use, such as functions, procedures or macros, which can be defined on-the-fly as part of a larger expression without explicitly having to define that code unit as a separate function, procedure or macro. It is especially not necessary to define a symbolic identifier when creating a lambda object. Lambda objects are exceptionally useful as small callback functions for GUI-elements like push-buttons, menu entries etc, because they allow to specify the callback in the same place where the widget itself is defined. This can be used to implement widget-creation code and event handler callbacks as a self-contained unit.
A closure is a special type of lambda object. It is a block of code (named or unnamed) which can typically be accessed by some reference variable and has some "environment" bound to it, such as variables from within the scope the code block reference has initially been taken. Closures usually allow extending the lifetime of the "inherited" scope variables until the lifetime of the closure.
* Rust has two types of closures: "Stack closures" which have a lifetime limited to their scope of declaration, and "boxed closures" which can live on independently from the scope they were created. The latter closures require dynamic allocation and are less efficient to create than the former ones. Boxed closures create private copies of the values from the bound variables; i. e. they detach from the original stack frame and cannot therefore see updates of the original variables or modify the originals. There is even a third type of closures, "unity closures", which are like boxed closures but in addition they ensure that no variables are shared with other code at all, and not just variables from the stack frame. This feature is used for spawning closures as new tasks. There is also a short-hand syntax for creating closures on the fly from inline-expressions.
* Go supports lambda functions. Functions are full closures and can be passed around like any value.
* C++ does not support lambda functions. The preprocessor can be used to some extent to emulate ones, as featured by the "event map"-style macros of various widget frameworks, but none of those emulations can really substitute for the power and elegance of native lambda functions.
* Perl has full support for lambda functions. Just use "sub" without a name as part of an expression. The resulting value will be a reference to the code block following the "sub". Proven to be very handy in Perl/Tk or simulating a "switch" statement by using a hash with lambda functions as values.
* Python has "lambda expressions", but they are rather a bad joke than something to be taken seriously. Especially their restriction to a single logical line makes using them awkward. They are only useful for a very small lambda expression code size, and cannot contain any statements. No match for Perl's "sub".
* JAVA has lambda-like functionality by allowing to provide interface-implementations inline without a need to write a separate class for this elsewhere. Unfortunately, the syntax for this is rather awkward, and it barely saves typing compared to a full class definition. But at least it saves the developer from figuring out another rather useless single-use identifier.
* Ruby does not have plain lambda functions, but has (both more powerful and more expensive) lambda closures instead which can be created with the "proc" constructor and be invoked with the "call" method. The "proc" constructor can also be invoked implicitly by using a "&"-prefix for the last formal parameter of the method.
* Erlang provides the "fun (Arglist) -> Expressions end."-syntax for defining lambda function expressions. Already existing functions can also be used in place of real lambda expressions using the syntax "fun FuncName/Arity". In both cases, the result is a code reference which can be used instead of a function name when invoking the function.
* Ada seems to provide neither unnamed functions nor closures with bound variables. However, it seems to allow local functions like PASCAL which share variables with the enclosing subprogram.
* Tcl has (at least as of version 8.5) the "apply" command for using lambda functions. Tcl can return and store code to be evaluated later as strings, which can also be used to achieve a similar effect as lambda functions. There are no real closures, but the evaluated code can refer to variables in the current context by name. It is also possible to refer to outer contexts using the "uplevel" command. As an alternative approach often implemented by Tcl object-orientation extensions, Tcl namespaces can be mis-used as container for closure variables. Namespaces also provides a namespace command for wrapping an arbitrary user command such that the variables of a particular namespace can be seen as global variables from within the user command, although they are actually separated in their own namespace.
* Wren supports lambda closures which can be passed around and stored in variables. This does not make the variable "callable" as if it were a function, though. Instead, the built-in call() method must be invoked on the lambda object in order to execute and pass arguments to it. When calling a lambda object this way, Wren does not even care if more arguments are provided than necessary. It only complains if less actual parameters than formal parameters are provided. This makes lambda calls different from methods invocations, which have a signature for overloading based on the number of arguments.
* Julia has Lambda functions which look just like in Lua.
* Crystal features objects of type "Proc" which are unnamed closures.

Operator overloading
Some languages like C++ allow redefining the functionality exposed by symbolic, built-in operators. This allows to use those operators on custom classes in a way which resembles the use of those operators in built-in standard types such as integers or floating point numbers.
* C does not allow operator overloading at all. Neither possible is function overloading, except for a one single case: A "static" function defined at file scope can hide an external function with the same. However, this only works if no declaration of the external function is visible to the compiler when the translation unit is compiled.
* C++ allows overloading of most operators, including conversion operators. The overloaded versions have the same arithmetic evaluation precedence and associativity as the original operators. The set of supported symbolic operators is static - it is not possible to introcuce new, custom-defined symbols that way.
* Objective-C does not allow overloading of operators. The internal symbol references generated by the compiler from message names in the source code for representing them to the runtime system are called "Selectors". Selectors encode only the name of messages and the names of their arguments, but they do not encode method parameter- or return types.
* Perl allows operator overloading using a pragma. Due to the constantly changing interfaces of pragmas between Perl versions, this features is not commonly used except for in heavily maintained extension classes.
* Ruby allows overloading of operators for all objects, and even allows changing the number of arguments in some cases. Some operators like "!=" are Ruby-internal macros rather than independent operators, transforming expressions like "a != b" or "a += 1" into actual expressions like "!(a == b)" or "a = a + 1" respectively. In such cases, only the actual operators can be overloaded; the internal macros always do the same conversions.
* Python allows overloading of operators by mapping associated symbolic names (embraced within pairs of double underscores) to operator symbols. Defining functions using the symbolic names redefines the respective operator.
* Erlang does not allow operator overloading, but at least it allows function overloading based on the number of arguments. It also allows to provide patterns and guard expressions for a set of function clauses of the same arity, where the patterns and guards determine which of the function clauses will actually be selected among the list of candidates with same function name and arity.
* Ada can overload subprograms, but seems to have no support for overloading operators.
* Ceylon does not support operator overloading directly, but it can be achieved indirectly by overriding methods with predefinied names in classes derived from an appropriate system classes. For instance, "*" will always be the multiplication operator and can only be used on objects derived from a numeric system class. However, every class can be derived from a numeric system class it need to, and then a user-provided override for the multiplication can be defined. However, the arguments and return value of the override must conform to the defined system interface. This means that operator overloading is in fact possible, provided that the override operates closely to the the semantics of the original. An operator doing something completely different from its original meaning cannot always be implemented in ceylon due to argument type restraints. This is not necessarily a disadvantage, as operators can always be expected to behave in the same way, even if they are specialized overrides.
* Tcl has no operator overloading because it has no operators. Expression evaluation is only done within the "expr" command. If needed, a custom-replacement for the "expr" command can be creates which interprets the operators differently. Such a command could easily be implemented as a preprocessor which replaces the operator symbols within the expression, and then passes the result to the built-in "expr" command.
* Rust allows operator and function overloading via traits.
* Wren allows overloading of most operators by using them as function names. The different arity of operators is mapped quite nicely to methods and getters. Overloading is generally based on function name and function arity.
* Nim allows to overload most operators. Only some are restricted to avoid possible confusion. Both infix and prefix operators can be overloaded, but not postfix operators: There are no postfix operators in Nim at all. Overloading uses the actual operator symbols as procedure names for the overrides, quoting them with backquotes which makes the symbols usable as identifiers.
* Crystal is fully object-oriented, and every class can define most standard operators in a custom way.

K. O. criteria *against* using a particular programming language
* D - The "auto" keyword disaster, compiler of the reference-implementation is closed-source
* Ruby - "yield" in a constructor is no viable replacement for a synchronous destructor. Exception handling via "ensure" (a.k.a. "finally") keyword is not a real replacement for missing destructors. All in all, although RAII is possible to some extent in Ruby, it is error-prone to implement, inconsistently designed, and does not scale well (because each "destructor" requires code block nesting).
* Ruby is notoriously slow. While it is not as slow as a snail and there are in fact other languages which are even slower, it is unlikely to ever win a price for its speed.
* Python. Overuse of underscores and particularly ugly string quoting (such as by triple-quotes). Also, its "lambda" operator is not a full replacement for truly anonymous functions because it cannot contain statements. There is no way to explicitly create different scopes for variables within a function. Another major problem of the language is how language revisions are handled, or better: Not handled. There is no easy way to determine which language standard is used for a particular existing script or library other than "to just know" it.
* If you still consider using Python, read PEP 8 before you decide. Be certain to have your barf bag ready when you try to understand the logic why "i + 1" and "(a+b) * (a-b)" are good style while "i+1" and "(a + b) * (a - b)" are bad style.
* Perl does not have generators or co-routine support yet. It uses a lot of special-purpose predefined global variables. A very large number of functions is directly built-in. A lot of special syntax has been added for very specific use cases, such as Perl "formats" or "plain old documentation". But Perl has everything else. On the other hand, Perl code tends to become unmaintainable unless explicit care is taken to avoid that.
* C++ is a language similar to Latin - it seems to have more exceptions than rules, is hard to learn (the ISO standard alone is more than 1000 pages), and many of the existing rules are rather of the type "because Bjarne said so" or "any rule is better than no rule" than being logically conclusive. But the worst problem of C++ is the standard-non-conformity of most existing implementations. For instance, as of 2011, there was only a single compiler implementation out there claiming to conform to the 1998 standard... and even that compiler was a commercial one, not available to the public for free. As a consequence, many C++ applications avoid newer features of C++ such as namespaces, templates, RTTI or exceptions, because the portability of those standardized features is practically very unsafe among existing implementations. Another problem of C++ are incomprehensible error messages from the compiler, especially regarding the actual location of the error. This is normally a by-product of too many levels of template expansions in an application.
* C - This language is still the language of choice for writing portable native applications (besides COBOL perhaps). But better beware of it for anything else - it's just too outdated by today's standards. On the other hand, it's easy to learn (but not to master), fast and the ANSI-C 1989 standard (as opposed to the ANSI-C++ 1998 standard) is implemented nearly everywhere. Even the newer 1999 Standard is now (as of 2015) implemented almost everywhere.
* FORTRAN - the original language must have been an invention by Italian spaghetti lovers. F77 has added a lot of syntactic features giving it at least the look of a post-dinosaur programming language. But don't let yourself be misled: It isn't. It still has no stack. No real "local" variables. No recursion or re-entrancy. Multi-threading not to speak of. It's like a low-level assembler language without the power of real machine language. But the simplicity of the language (for the compiler - not for the developer) has its advantages: It can be optimized and even parallelized in phantastic ways. A good optimizing FORTRAN compiler still outperforms any other languages for number-crunching applications. So, if speed is your utmost concern and number-crunching is your application's primary domain, consider FORTRAN. But run from it like hell for any other type of usage. Update: Actually, that's no longer true. The most modern FORTRAN provide extensions for recursive programming and even dynamic memory allocation. However, those modern dialects are not as widely employed as the FORTRAN-77 standard as described above. Also, adding those features defeats the actual purpose of FORTRAN to be as fast as hell for the price of looking ugly. But its raw speed is the only reason to use FORTRAN at all. There are plenty of better languages out there when a general programming language is wanted rather than the fastest possible one (besides assembler- and machine-languages of course).
* JAVA - the language praised for its portability. Actually that's a lie: JAVA is not a cross-platform language, but JAVA is rather a platform itself. This difference is subtle, however, and in most cases JAVA's promise of portability holds. JAVA is a very readable language - programs tend to get very verbose with little chance for "programming tricks" or syntactic exploits. But this has a price - the language is very simple, the programmer has to type in much more code for gaining the same effect other languages provide with a fraction of that code size. But this makes JAVA a good choice for big corporations which plan to hire and fire their developers on a daily basis - there is simply less which can go wrong if a developer is to be fired and a new (cheaper) one taking his place. The largest disadvantages of JAVA - from a technical view - are its lack of support of RAII (it has no synchronous destructors) and its lack of abstraction. Also the different sets of "standard"-libraries for graphical user interfaces are a constant source of trouble - but that's not something the language itself is to be accounted for.
* Erlang has no native support for any particular string encoding except Latin-1. Actually, it does not even have any character string support at all: Strings literals are considered syntactic sugar for a list of integer values. This means Erlang is a bad choice for string manipulation, especially if the strings get large and Erlang's linked character list representation will become a deeply nested data structure (each additional character adds a nested sub-list to the list representing the string). There is also little or nor support for collation sequences, locales etc. Erlang's command-line parsing is heavily non-standard, which sometimes can make it hard to write programs which conform to the usual commmand-line parsing standard of the host OS. Expressions are sometimes limited to built-in functions. For instance, it is not allowed to call user-defined functions from within the conditional of an "if". In such cases, temporary variables must be used. Erlang has no direct syntactic support for hashes or arrays, although library modules provide equivalent functionality. However, this makes hash- and array-operations normal function calls, which can lead to overly verbose code (like in LISP). Erlang is not object-oriented, and the only means of emulating such functionality is using multiple processes or container types which store a list of all "object instances" a function operates upon.
* PHP: Weak dynamic typing (strings which look like integers are always compared as integers, even if they are too large to be interpreted as an integer and truncation occurs). Integer truncation is actively checked for and does not occur as the usual modulo 2**n truncation: Instead, the number of digits visible to the conversion function is silently reduced until the conversion succeeds, and no error, exception or NaN-value results from this. Uncommon precedences of established operators (ternary operator looks like in C but has different precedence). Arbitrary restrictions of syntactically allowed expressions (it is not possible to access array elements from arrays returned by a function). Barewords: Unknown identifiers are automatically interpreted as strings; no warnings occur.
* Ada is a PASCAL-derivative. It therefore uses "=" instead of "==" for comparisons, which alone is enough to make it impossible to use for developers which have to use C-like languages at the same time in different projects. Languages which use "==" are C, C++, C#, JAVA, JavaScript, Perl, PHP and many more. Working in any of them and Ada/PASCAL at the same time will inevitably lead to chaos when "=" is used where "==" is appropriate.
* Ada requires all source files to be recompiled if a source-file depends on other source files. This is even required for internal changes in dependent source files where the interface does not change. This contradicts the conventional use of shared libraries where recompiling is necessary only for "major" updates of the library which change existing functionality in a non-backwards-compatible way.
* Ada is very powerful, but it is a large language comparable in complexity to C++. This combined with the fact that it has a rather small user base and that there are potential licensing issues makes learning the language less appealing then say C++ which has at least a large user base. Also Ada programs seem to store string representation of all enumeration values for all enumeration types used in the program, which alone can lead to bloated executables considering how many such constants are defined in typical library interfaces. Another problem is that the language uses overly long names for very frequently used types such as "Unbounded_String". Types can be renamed locally, but the long names have to be used at least once in the renaming statement. Generally, one has to type easily twice the amount of text in Ada than in C++ because of the more verbose syntax and standard library item names.
* Vala might be a nice language, but there is just no reason to use it unless you have to. It is syntactically similar to JAVA/C# and similarly easy to learn, but does not share the security features of those languages. In particular, it cannot automatically detect buffer overruns - it is no more secure than plain C (meaning there is barely any security at all). It is definitively slower than C++ and has also has a slightly larger memory overhead, because it stores reference counters in structures even in cases where this is not actually necessary. It will in practice also be slower than C Code, because it uses reference counting far more extensively than manually-crafted C code will ever do, and also in situations where it is just not necessary. It depends on glib, which adds just about the same byte size of total basic runtime library dependencies as C++, which is approximately 3/4th of a megabyte more than what C requires. Vala also does a bad job in symbol hiding, exporting far more symbols to the linker than are necessary. It also adds superfluous diagnostic messages to the executables which cannot be disabled even for release builds. Summing up, it is less powerful than C++, less secure than JAVA/C#, and generates code which runs slower than normal C/C++ programs. In addition, it has a very small user base, and the language is not even finished. Considering that gjc can also generate native code from JAVA source files, there seems no reason for using Vala at all. It is not bad, but inferior in any single aspect when compared to much more popular languages like JAVA, C# or C++. The only justification for Vala's existence seems to be the fact that it was the easiest way for the GNOME project to migrate their macro-infested and nearly unmaintainable C code into something more managable. Also, it allows them to keep their pride, because they proved "C can do the job, we don't need no stinkin' C++ which we have always hated. See, we don't come crawling back to C++!"
* From the viewpoint of the language itself, Ceylon has little or no disadvantages compared to native JAVA when generating bytecode for the JAVA VM. However, its execution speed just sucks - at least on less powerful computing platforms. On a Raspberry Pi with 512 MB of RAM and a rather weak CPU, for instance, it took over 2 minutes to compile a simple "hello world". And it also took nearly one minute to just run the compiled executable. The Pi is certainly slower than an average desktop PC, but it is not *that* much slower.
* Although newer Tcl implementations actually convert the code to be interpreted into bytecode internally, this is not always possible due to the fact that "everything", including code, "is a string" in Tcl. Tcl needs to re-parse code containing substitutions at run-time, updating its byte-code caches, depending on where substitutions are present and how frequently the substituted values are changed. This means that Tcl scripts normally run very slow compared to compiled languages, but can still be expected to run much faster than UNIX shell scripts (as long as the startup overhead of the Tcl interpreter can be neglected, i. e. if the script actually performs some amount of internal processing). One problem of Tcl is that the core language has neither closures nor object-support. Both can be emulated using "namespaces", which are actually entries of a single global hash table which can be accessed by name via language support. The only way to implement instance variables of objects or closure variables is to store them in such namespaces of in global hashes and index them by namespace name or hash index in some way. The problem with this is that Tcl provides no way to associate a reference to such an instance variable permanently to a user command. While "namespace eval", "upvar" or "my" seem to make this possible, those constructs are actually runtime commands which create aliases for the respective global variables in the scope of the current command. Which means they consume processing time every time a command is called, not just when the command is created. And even worse, this overhead is directly proportional to the number of aliases created, no matter whether the aliases are actually accessed or not by the code. And every time a command returns, its stack frame is destroyed, and all the aliases just created together with it. This can create considerable administrative runtime overhead for method invocation. Furthermore, I see the same problems regarding the plethora of standard command and their options as for LISP, especially regarding control flow commands. Most of them have a lot of options, and every option may change the functionality of the flow-control command in non-obvious ways. Readability may also be impaired by the fact that many "keywords", such as "then" or "else", are (depending on the context) completely optional, and may therefore be omitted. In such code then only the order of arguments matter, but this order is not self-explaining. It can therefore be hard to guess what control flow commands actually do, unless you know all commands and their options by heart. Next, I personally find Tcl scripts hard to read, especially when many code blocks are nested deeply within curly braces. It is then often hard to distinguish which is actually code and when what part of the code will be evaluated or substitutions will be made. Tcl is not worse to read than Perl, but clearly harder to decipher than languages with fixed control flow keywords. Finally, I found it amazing that while Tcl throws an exception for almost every runtime error, these exceptions are not necessarily created reliably for every possible I/O error. And even when they are, the exit code of a Tcl process does not necessarily reflect that. For instance, writing a string to /dev/full aborts processing with an error message, but then exits the Tcl script with a success exit code. Custom error handling might be able to fix that, but it is a strange default behavior.
* I do not have many reservation against Lua as a programming language itself, but its standard-library severely sucks when it comes to error handling. Most I/O functions to not raise exceptions when I/O errors occur even though the language itself supports this, but rather require the caller of the function to check for the error manually, like in C. This is sometimes called the "nil protocol", meaning that error conditions are returned as two values, "nil" (a special value like NULL in SQL), and an error message. This usually means that most I/O function calls need to be wrapped inside an assert() call, which throws an exception once the "nil protocol" indicates an error. Obviously, this is an error-prone approach, because it is easy to forget putting the assert()-check in place. But even worse, not all I/O functions actually follow the "nil protocol" - some actually throw exceptions. Others ignore I/O errors completely - for instance "print", one of the most-frequently used functions in Lua scripts. While it is true that all those problems do not apply as long as Lua is used purely as an extension language and the I/O functions of its standard libraries are not used, it makes Lua hard to recommend as a general-purpose scripting language, where I/O errors should normally abort a script rather than being silently ignored like by Lua's "print". In order to use Lua as a general-purpose scripting language, most of its "io"-library needs to be replaced by a new library with more reliable error checking and reporting.
* Most Lua-libraries written by third-party developers follow the conventions of its "nil-protocol" for error handling, and therefore suck likewise, leading to error-prone code and tons of assert() calls instead of throwing exceptions. It is really hard to understand why no-one in the Lua community seems to see the problem, especially as the language does support exceptions. They are just not usually used for error reporting. Maybe performance is a reason, because catching exceptions requires setjmp() at the C runtime level. On the other hand, the alternative of nesting most function calls in Lua programs within assert() calls certainly will not benefit performance either.
* LISP. My objections against LISP are primarily targeted against its syntax, which makes indentation awkward, following seemingly arbitrary rules which differ between standard functions and can be hard to see because the indentation sometime differs only by the width of a single parenthesis. Another problem is that frequently required expression functionality such as indexing an array or hash requires overly long source text constructs. But the main problem is actually the flexibility of the language, allowing users to add their own control-flow functions if they want. A drawback of this flexibility is that casual readers of source code typically have no idea which argument of a function of control flow construct has which purpuose. This is aggrevated by the fact that the standard library is huge - hundreds of functions. It propably takes years to remember all that. As a practical problem, LISP systems tend to be very large and resource-consuming, just like JAVA. And like JAVA, there are exceptions. But they are few. Normally, installing a LISP systems consumes hundreds of MB disk space. And you better have a lot of RAM, because like most GC-based languages, LISP's GC is based on a memory-speed-tradoff.
* Due to its old age, SNOBOL has ridiculous limits compared to more modern programming langages. Integers only support the range from -32767 to +32767 and strings cannot be longer than 5000 characters. At least, 8-bit characters are fully supported. Another problem is that packages for SNOBOL are not normally available in typical Linux distributions, so one has to compile it by oneself. SNOBOL was in wide use until the 80's, when regular expressions of Perl and AWK were used for the same sort of problems instead. But SNOBOL is strictly more powerful than POSIX regular expressions, because it can be recursive. However, since Perl 5.10 recursive regular expressions are also supported.
* Rust seems to create large executables compared to C. A simple "hello, world"-program created a 2 MB executable and was still 300 kB when stripped of debugging symbols. Therefore, Rust does not seem to be a good choice for small utilities. For large applications, the size differences will eventually become irrelevant.
* Go provides "++"/"--" symbols which look like postix operators but are actually statements. This is inconsistent with C/C++ where those symbols are used as operators and there is a semantic difference between the prefix and postfix variants of those. Forcing the developer to disregard those differences and always use the postfix-variant is *bad* if you need to use both languages at the same time in different projects.
* Go enforces formatting details which other languages do not care about. For instance, line continuation is only possible by placing the operator at the end of the line, and does not allow placing them at the beginning of the next line. This will force people who are accustomed to other formatting conventions to use formatting rules which they might be hating for good reasons. But even worse, they might get accustomed to Go's conventions (because there is no alternative there), and will then be confused when working with their old source code in other languages.
* Go has a code formatter which enforces very specific formatting, such as the amount of indentation (exactly one horizontal tab), where to set spaces etc. The formatter even does childish things like horizontally aligning assignment operators, which will create useless deltas in version control when a single line changes but the formatter will reformat adjacent lines as well in order to align the operators. In addition, the formatter does not care if a line gets longer than 80 columns due to its actions. Although usage of the formatter is "voluntarily", it can be expected that everyone will assume Go source code which is formatted differently a "bad style". And your boss will likely force you to format your source code that way. This might be OK if you are only Go and no other programming languages where you are accustomed to different formatting styles. But this case is actually very unlikely. Go has not conquered the world yet, and it seems unlikely that it ever will because most experienced programmers have their individual formatting styles and will rather choke on a language which enforces them to use a different style rather than adopting that style.
* The fact that the Go formatter indents in units of one horizontal tab means that only very few levels of indentation are possible before the right margin will be reached. This forces the programmer to subdivide the code into smaller functions for no other purpose than avoiding deep indentation, which will make the understanding of complex code harder to follow due to diminished source code locality.
* Even though the Go formatter indents only in units of one horizontal tab, the Go online tutorial suggests that the tab width of the editor is expected to be set to 4 columns. While this alleviates the beformentioned problem somewhat, it adds the complication that not all editors allow to customize the tab width. Also, when source text is displayed in the terminal, it will use the terminal's standard tab width of 8 instead of 4, unless "expand" or other tools are used to preprocess the source code before display. This is actually very annoying. If Go wants to indent by 4 columns, it should use 4 spaces to do so rather than using a tab and assuming a non-standard width setting for it.
* The Go formatter has rather arbitrary rules. For instance, the second argument of the built-in function make() formats the same expression differently (without spaces) as it would format the same argument for a normal function (spaces between operators).
* Go's error handling just sucks. One has to plaster the code with zilliobn of "if err == nil"-comparisons.
* Go is rather slow. C is more than 3 times faster using the same compiler backend and optimization settings, at least in a benchmark with a lot of matrix manipulation and linear algebra. This might even be slower than JAVA.
* Go's garbage collector seems to assume everyone has memory overcommitment enabled. It deliberately allocates much more memory than it intends to actually use. This could become a problem on servers which disable memory overcommitting because they want to avoid the OOM killer at all costs.
* As of 2019, Go creates version hell. Not only for the source texts which bear no indication for which version of Go they were written. But also for the documentation, because Go does not ship with local documentation. And the documentation on the Web Site is always much newer than the Go version you installed using your package manager. Although it is theoretically possible to build an older documentation set locally, this is very complicated because it requires a matching version of the Go compiler, which one of course will not normally have available for installation. So a matching Go compiler needs to be built first. Unfortunately, the rather old Go branch 1.4 is reqired to start the bootstrap process of building a newer Go compiler. And building 1.4 from source requires building a particular gcc version with a matching Go front-end. Bootstrapping gcc is no fun even for C, and there is a lot which can (and usually does) go wrong. Expect build failures, required patches - a whole lot of fun. For a masochist.
* Wren does not allow continuation lines to begin with an operator or comma. This is because of the simple rule that newlines within expressions are ignored if the tokens before the newline do not form a valid expression. Unfortunately, this normally only applies if the line ends with an operator or comma.
* Wren has only a single numeric type, which is C "double"s. Lua made the same mistake once, but has fixed this in Lua 5.3. Doubles are expecially problematic for file offsets when dealing with very large files.
* TypeScript itself might be a useful language, but the required compiler and most of its related toolchain are based on the "npm" package manager. npm automatically downloads and executes JavaScript code from countless sources onto your machine and runs it there without any sandbox protection. There is no trusted review process involved and thus no guarantee, that some of the installed dependencies do not contain malware. And even if they don't, this could change at any time. Which means the only way to use TypeScript is within a VM. And even that is dangerous because VMs are not foolproof and typically have plenty of bugs and security problems.
* Julia seems to be a very nice language. There is little to nothing to complain about the language itself. The problem is its implementation footprint. No less then 300+ MB shared library dependecies are necessary to even get the command prompt going. Likewise, it has a virtual memory allocation size of also about 300 MB RAM, and a primitive test application reading lines and printing them out again required about 130 MB RAM. This might be acceptable in a research project where the whole machine and its software follow a single research goal. But it is certainly not something you want to replace your Perl-scripts with. In short words, Julia is way too fat for everyday use as a general-purpose programming language. A pity, because otherwise Julia seems to have the potential of being a very fine everyday language.
* Wren's standard library is too minimal in several regards. At least when using the standaline CLI implementation. It does not even provide a way to execute an external command - something even AWK provides. This might be good for being an extension language, but it sucks when actually using it as a scripting host.
* Kotlin seems to be a potentially nice language, but it it is hard to get a trustworthy implementation. It is rarely available as a distribution-provided package, and so one has either to download the pre-build binary (which is easy but dangerous) or to build is oneself (which is hard and resource-consuming due so the many dependecies such as multiply JDK-versions which need to be installed side-by side). Also, the build is driven by gradle which happily downloads even more build scripts and rund them, which is dangerous as well. Therefore, there seems to be no easy way to get a trustworthy version of the kotlin compiler without endangering the integrity of the local system.
* Scheme is nice. However, citing the Wikipedia (2021-01): 'The Scheme language is standardized in the official IEEE standard [...] has a diverse user base due to its compactness and elegance, but its minimalist philosophy has also caused wide divergence between practical implementations, so much that the Scheme Steering Committee calls it "the world's most unportable programming language" and "a family of dialects" rather than a single language.'
* The "delay" and "force" procedures of Scheme which allow lazy evaluation and "promises" have been removed from the core language and are now implemented as library functions. I did read about this implementation being based on memoization, i. e. keeping a copy of all previously evaluated results in memory in order to avoid evaluating them again later. This might be the right method in some applictions, but it seems hairbrained to me to do this in general for *all* delayed evaluations. (On the other hand, I might be wrong and might have understood the implementation incorrectly.)
* In the particuar case of the popular Scheme implementation GNU guile, two controversial design choices are known: "call-with-current-continuation" is implemented by copying the "C" runtime stack forth and back, which does not seem to be a particularly performant approach. Also, it is based on the BoehmGC garbage collector, which cannot guarantee that all memory will be freed during garbage collection due to the possibility to mis-recognize random patterns found in memory as object addresses, which will not be garbage-collected as a consequence.
* Line continuation in Nim works exactly the wrong way around, at least due to my preferences. Long lines can only be split after an opening parenthesis, an operator symbol, or after a comma. I can live with the parenthesis, but I hate putting a comma or operator at the end of some line.
* As of version 1.7, Crystal uses the BoehmGC garbage collector. This is a very inefficient GC which may lock garbage in memory just because some bit pattern in memory looks like a pointer to it. On the other hand, Crystal wants to be faster than Ruby, and Ruby is slow as a snail. Maybe not even BoemGC can stop Crystal from achieving its objective.

Functional expressions.
Some languages allow expressing things like loops, synthesizing lists and arrays by repeated rule applications and similar functionality as part of normal expressions, rather than introducing special statments or forcing the developer to write specialized callback functions for such purposes.
* C++ is poor at the language level, but the STL library provides some very powerful tools in order to emulate functional behaviour. Unfortunately, STL is highly complex, hard to master, and cannot detect many errors before runtime, because STL's abstract concepts exceed the actual power of C++ by far and are thus only specified loosely as (mandatory) "conventions" rather than something the compiler can check.
* C has no special support for functional programming.
* Perl has some support for frequently uses of functional programming, especially its "map", "sort" and "grep" operators. But although those operators are exceptionally useful, they cannot replace real generators as provided by other languages.
* Python has "map", "filter" and "reduce". "map" applies a function to each element of one or more (parallel) sequences. "filter" filters out any elements from a sequence not matching some condition. "reduce" allows to express accumulating operators like sum and product of all elements in a sequence. Python also has "list comprehensions" like "[2 * x for x in range(10)]" which expand to lists, and "generator expressions" which look the same, only without the brackets. (Actually, I assume this to be the same: List comprehensions just initialize an array using a generator expression, which will generate all the array elements without further delay. Generator expressions when left alone, on the other hand, will only execute when the next element of the sequence is actually requested.)
* Ruby has full support of generators and is thus capable of doing anything Perl can do in this field of application, and much more. But its generators are not as powerful as real co-routines, because the "yield" statement can only transfer control into and out from a single block of code.
* FORTH does not make a distrinction between functions and statements; basically the whole control flow could be viewed as some sort of a large single expression.
* Being a single-assignment functional language, Erlang supports all aspects of functional programming, including lambda expressions, tail recursion, list comprehensions etc. Generators and iterators are implemented using its built-in lightweight userspace-scheduled processes, which will then be used in a coroutine-like manner.
* In Rust, everything which is not a declarations is an expression. This includes if, while etc. Generally, the last value of a block is its "result". However, it is possible to force a block to return "nil" as a result by terminating the last statement within the block by a semicolon. A "return" statement is available, but it is only intended to be used to return from somewhere within a function sooner than by "naturally" reaching its end.
* Ada does not seem to have any special support for functional programming beyond using callbacks for this purpose.
* Tcl has no special provision for functional programming, and its lack of non-symbolic references makes it even harder to implement such a thing. On the other hand, Tcl considers everything as a string, which includes code as well as function names. Therefore, implementing things like "map", "filter", "reduce" etc. is certainly possibly, but those constructs might be very inefficient if they require incremental modification of nested data structures which need to be serialized as strings in order to be passed. Flat data structures are usually not a problem.
* Wren has a full suite of built-in functional methods for sequence types, such as map(), join(), reduce(), each(). In addition, it has range objects which can be instantiated by range operators and act as iterators. It is also easy to implement iterator objects oneself. Although being highly functional, Wren in fact does not support conventional plain functions at all! It only provides member functions and lambda closures. The closest thing to a conventional function in Wren is a "static" member function.

Boolean values.
Such values are the result of boolean expressions and will typically be used as conditions.
* C: Expressions of various numeric types will be auto-converted into booleans. Null-pointers are interpreted as "false", any other pointer-values as "true". Numeric expressions will be converted into integer values, and each integer value not equal to zero is interpreted as "true". C does not have a dedicated boolean type; comparison operators generate "int" values 0 for "false" and 1 for "true".
* C++: Mostly like C, except that a type "boolean" with constants "true" and "false" does exist there. But using "boolean" is optional; C++ works the same as C if numeric values are specified as conditionals.
* Ruby: Any value that is not "nil" or the constant "false" is true. The number zero is not interpreted as a false value. Neither is a zero-length string.  Comparison operators return constants "true" or "false". Operators which return either a value or a failure indicator return that value or the constant "nil".
* Perl: Any value that is neither "undef" nor "0" (both as a string or as a number) nor an empty string is true. No further type conversion or -coercion is performed upon comparison! Comparison operators generate 1 for "true" and undef for false.
* FORTH uses BASIC's semantics: An integer with all bits 0 is "false", everything else is considered "true". A canonical "true" has all bits set to 1.
* Erlang uses no dedicated type but just the atoms "true" and "false" for canonical representation of booleans. Actually, anything not equal to the atom "false" is considered to be true.
* Ada has a boolean type, but it is actually just a predefined enumeration type with additional defined semantics when used in conditional expressions. It is possible to derive customized types from the predefined Boolean type, which will then also inherit the special semantics which allows to use them in conditions.
* Tcl has no operators and everything is essentially a string, so every command is free to define what string arguments it will be interpret as "true" or "false". Obviously, this is not a very fortunate situation, and sometimes several different representations of "true" and "false" are supported by a single function. It certainly makes learning the whole language not easier, because the different variants need to be remembered for specific commands.
* Rust has a "bool" type with the two values "true" and "false".
* Although Julia does provide a "Bool" type with the values "true" and "false", those are automatically promoted to the numeric values 1 and 0, respectively, when used in arithmetic expressions.
* Wren follows Ruby's rules for interpreting values as booleans: The values "null" and "false" are false, everything else is true (including numeric zero and empty strings).
* Lua also interprets "nil" and "false as false and everything else as true.
* Crystal interprets "nil", "false" and null-pointers als "false" and everything else as "true".

Numeric literals.
Most language consider a sequence of decimal digits to be an integer literal. Sometimes a negative sign is part of the literal, in other languages it is just a unary operator. Some languages also allow to group digits using separator characters which are otherwise ignored.
* JAVA allows optional 0x (base-16) and 0b (base-2, since JAVA 7) prefixes, L (preferred) or l (long rather than int) and F or f (float rather than double) suffixes, and (since JAVA 7) underscores in the middle of a literal for digit grouping. Hex digits can be upper- or lowercase. E or e can be used to specifiy the exponent in FP numbers. The decimal point is always ".".
* Wren only supports double precision floating point numbers, but the symbols "true" and "false" can also be coerced into ones. In addition, 0x prefixes for hexadecimal values are supported.

Bit-parallel operators.
Some languages provide operators which perform boolean logic operators for all bits of the binary representation of the operator arguments in parallel. This can be used to emulate arithmetic on bit-fields. It also allows several programming tricks which can significatly speed up some calculations.
* C and C++ provide bitwise "and", "or", "exclusive or" and "not" operators. There are also shift operators which can shift all the bits of a value to the left or right. The bits shifted out are lost. The shift operators are only well-defined for unsigned values which will shift in new "0"-bits to compensate for the bits shifted out at the other end. Performing shift operations on negative values has undefined effects. It is also not clearly defined whether it is valid to shift by more bits than there are actually present in a value, of whether shifting by 0 bits is allowed. Such operations must be avoided in portable code.
* Ada provides the bitwise "an", "or" and "not" operators. For some built-in predefined integer types, bitwise shift and bitwise rotate-operations are also defined (as library functions).
* Before Lua-5.3, bit operations were only available as library functions, and worse, they required computionally rather expensive floating-point/integer roundtrip conversions. Since Lua-5.3, Lua provides native 64 bit integers and includes the usual bit operations as an integral part of the language.
* Although Tcl does not have any operators built into the language itself, its "expr" command supports most of C's operators including all the bitwise operators.
* Julia has all the bitwise operators of C, although some of them use different symbols. Other than C, however, Julia provides both logical and arithmetic shift right operators.
* Wren provides the same operators like C, except that they will be temporarily converted to unsigned 32 bit integer values before carrying out the bit operation.

Short-circuit boolean evaluation.
Some logic operators such as "and" or "or" allow determinine the result of the operation without ever evaluating their second argument, depending on the evaluation of the first argument. This is because the value of the second argument cannot change the result in those situations. Languages which skip evaluation of the second arguments in those cases are said to do "short circuit evaluation".
* C, C++: Both use short-circuiting for boolean operators "&&" and "||", but not for bitwise operators "&" and "|". The short-cirtuiting operators always return 0 or 1, no matter of the actual arguments of "&&" or "||".
* Perl, Ruby: Both work similar to C/C++ with the important difference that the left argument is returned unchanged if it determines the outcome of the operation, and the unchanged right argument otherwise.
* Erlang provided the "andalso"- and "orelse"-operators for short-circuit evaluation in arbitrary expressions. In guard-expressions which are guaranteed to be free of side-effects, the normal boolean "and" and "or"-operators are also short-circuited as an optimization.
* OCaml's "&&"- and "||"-operators are short-circuiting like C's.
* Rust does short-circuit evaluation of "||" and "&&" like in C.
* Ada features the "and then" and "or else" operators which use short-circuit evaluation. The normal "and" and "or" operators always evaluate both of their arguments, however.
* Ceylon uses "then" and "else" in a similar way other languages short-circuit "&&" and "||". There is a difference, though, in that Ceylon's "then" and "else" operators only compare against null values, not ordinary truth values. This is primarily intended for handling "nullable" types, but can also be used to emulate C's ternary operator.
* Tcl has inherited most of C's operators, including those of performing short-circuit evaluation. However, only Tcl's "expr" command and the conditionals of control flow commands support operators at all, and those are primarily intended to use numeric operands. Nevertheless, "expr" supports C's short-circuit "lazy" evaluation, and can perform command and variable substitutions itself as part of that process.
* Julia supports the same shortcut-operators as C, including the ternary "? :"-operator. However, the detailed behavior is somewhat unique: Only genuine "true" and "false" are allowed as controlling expressions which decide the outcome. The expression which is actually returned can then be of any type, though. So "true && 5" is fine, but "5 && true" isn't because "5" is not an actual "Bool" value.
* Wren supports short-circuiting the "||", "&&" and "?" operators, like in C. However, the "&&" and "||" operators always return one of their arguments rather than returning an explicit "true" or "false" value. This also means those operators have special built-in behaviour and cannot be overloaded.

Enumerations, symbolic types and set types.
Some languages provide the means of user-definable symbolic values, such as "red", "green" and blue which can be assigned to variables and compared for equality. Some languages implement this by mapping symbolic values to integer values internally. Some languages even allow to specify such a mapping explicitly. Other languages implement symbolic values as members of a bit set, i. e. a bit position. In this case, set-operations like unison, complement or difference are frequently available.
* C/C++ provides the "enum" construct which by default maps idenfifiers to numerically increasing integer values, starting with zero. Although enums are different types than integers, they can be used exchangably in most situations. It is also possible to override the default assignment of integers partially or completely. C does not have provide set types or operations, but this can be emulated in many situations by using the bitwise operators to operate on bit sets kept in normal integer variables. However, this restricts the set size to the number of bits in the variables. The scheme can be augmented with some loss of efficiency to arbitrary sizes by interpreting arrays of integer values as segmented bit sets.
* Lua provides neither enumerations nor any other form of symbolic values. However, normal strings can be used as symbols. This is because Lua always hashes and de-duplicates strings, so all strings with the same content always refer to the same string object in memory. String comparisons (for equality) are there very cheap, as only the starting address of the strings need to be compared internally for equality. Using string literals instead of special symbolic types is therefore no performance disadvantage, neither in processing speed nor in memory usage.
* Ada implements enumerations similar to C, except that it is not possible to override the mapped integer values directly. However, it is indirectly possible by modifying the order in which the enumeration items are specified, as the first item always gets assigned the value 0, the second item gets value 1 and so on. There are operators for converting between the integer values and the enumeration symbols in both directions. In addition, conversions between the enumeration symbols and the string representation of the symbols is also possible in both directions. A practical disadavantage of this is that the executable needs to store the string representation of all enumeration constants - at least when the compiler cannot optimize it away. On the other hand, it is useful for debugging and diagnostic output to be able to always print out the value of a symbol displayed as a string. Quite uniquely, Ada enumeration constants are generated internally as functions which return the enumeration value. It is therefore also possible to rename the functions, and thus the literal symbols locally. The enumeration literals are also defined within the namespace of their enumeration type, which allows to use the same symbols in different enumerations. Name clashes arising from this can be resolved by explicitly qualifying the "symbol" function names with their package name. Also quite uniquely, Ada allows to use character constants in addition to symbols as enumeration literals. This automatically promotes the enumeration type to a character type, which provides string concatenation and allows to create character strings from a sequence of enumeration literal. It is also possible to declare subtypes of enumeration type which leave out some of the elements.
* Tcl does not have or need special syntax for enumerated types, because everything is a string, including numbers or symbols. Application code can compare strings as if they were symbols directly, or use them as hash keys in lookup table to associate them with numeric values. It would also be easy to implement some sort of "enum" command, which creates a hash for looking up numbers from enumeration constant names, or which creates commands from enumeration item names which return just "their" associated number. However, nothing of this is already part of the core language; at least not as of Tcl 8.6.
* Rust has "enum", but it only declares the items as symbols and does not associate any integers with them. The symbolic enum items are usually compared within a "match" expression, which can match patterns including symbolic names. Enum items are not restricted to be simple symbols, they can optionally have arguments, too. The arguments can either be named (as in a "struct") or unnamed (like in a tuple). Every enum item can use a different style of parameters, including none. Actual enum values can be constructed by binding parameter values to enum item parameters, and the beformentioned "match" expression also allows to extract those parameters from an enum item for actual use by the program. This feature is used by the Rust standard library to combine successfulness status codes with the actual return value of a function and return a value containing both pieces of information as a single composite return value. In this regard, enums are much like tuples, but enum fields are named while tuple fields are associated with their sequence order position inside the tuple. Other than in C/C++, Rust's enums are namespaced by the name of the enum type, and the normal rules for accessing items in namespaces apply.
* Wren does not have any of those. Enumerations can be approximated by defining getters which return the intended value. There are no symbolic types. Set types can be approximated by maps.

Composite and object types.
Most programming languages provide means of grouping fields into combined entities frequently called "structs", "records" or "objects". Object-oriented languages usually allow also to associate operations with objects, typically via other concepts like classes or prototype objects. Some languages support storage optimizations like "unions" for defining alternative structures which cannot be used at the same time and share the same memory locations. The only thing *not* described here shall be the subject of "getters" and "setters" for which there is a separate section in this document.
* C/C++ provide "struct" which allows to group fields (which can be other structs) into composite objects. Struct elements are accessed by name and not by their order. Both languages also provide "union" which allows to specify many variants of a single field which can be used to access the same storage space using different interpretations of its contents. The single field used in a union frequently contains different kinds of struct for the alternatives. struct values scan be initialized with initializer literals, based on the ordering position of associated fields within the struct. It is allowed to omit trailing initialization values in struct and array initializers, the remaining values will then be filled with whatever binary zeros represent when interpreted as initialization values of the remaining fields.
* C++ has C's structs and unions, but unlike C it also allows to associate operations with structs. C++ also has "class" which is the same as "struct" with different defaults for visibility and access.
* Rust provides "struct" much like C, except that struct initializers are always assigned via name and never via their ordering position. This means that exchanging the order of fields in a struct definition will never create problem for existing code which initializes those structs. In Rust, "enum" items with associated parameters can be used much like C's "union", except that they are tagged and the compiler and/or runtime will ensure that only the variant is used which has actually been created. This means, Rust's enum when used like C's "union", incurs storage overhead for a discriminator value.
* Wren has neither structs nor tuples. It can only abuse maps and arrays for that purpose, or define classes and provide setters/getters for accessing the "struct" fields.

Precedence of operators.
Taking the operators of "C" as a basis, analogous operators of other languages can be compared with them.
* Perl: In addition to operators "!", "||", "&&" and "^", the operators "not", "or", "and" and "xor" exist with the same function but lower precedence. In contrary to "^", "xor" ever returns only 1 or undef.
* Ruby is similar to Perl (as outlined in this document section you are reading), except that "and" and "or" have the same precedence.
* Rust has raised the precedence of bitwise operators, which allows to avoid parentheses in many usages of the bitwise operators, but also makes expressions containing bitwise operators incompatible to the majority of other "C"-like languages. IMO, this was a dangerous design decision which will *not* contribute to the success of this language.
* All operators which Tcl shares with C have the same precedence as in C.
* Tcl's "expr" command supports most of C's operators, and uses the same precedences where applicable.
* Julia has widely different operator precedences than C. They might seem "mathematically more correct" than those of C, but will certainly be a challenge when using C and Julia in turns. Julia also supports implied multiplication, such as "2x" instead of "2 * x". The implied multiplication has a higher precedence, so that "x / 2y" is interpreted as "x / (2 * y)". Julia provides several operators in addition to those of C, for instance for the square and cubic roots. Julia has support for complex numbers and literals, but chooses "im" as the symbol for the imaginary unit rather than "i" or "j". The reason is that "i" and "j" are popular variable names and the language designers did not want to deprive the users of them. Complex literals are normally specified with implied multiplication, i. e. "3 + 5im" rather than "3 + 5 * im".
* Wren uses mostly the same operators with the same precedence as C. However, the operator precedence of the arity-2 bit-parallel operators has been moved before those of the comparison operators in order to "fix" their priority. Expect possible problems there. On the other hand, most likely you will only make unnecessary parentheses by mistake. Wren also adds a couple of additional operators (in particular ".." for constructing inclusive ranges, "..." for exclusive ranges and "is" for type tests), but those have of course no effect on the precedence of the ones taken from "C".

Operators:
* Erlang is more biased towards PROLOG than being a C-descendant. Arithmetic and bit-operators are present as well as boolean and short-circuit operators.
* Perl has borrowed most of C's operators, and added a couple of additional ones. Especially interesting is the regular expression matching operators.
* Rust has also borrowed most of C's operators, but left out ++ and --.  Also, quite peculiarly, while the "==" is used for comparison in common, annotations use "=" as if it were a comparison operator. This is highly confusing and error prone. Actually, I surmise it is *not* really a comparison but rather the assignment of a match variable to be used in a comparison later; but it looks like a comparison anyway.
* Tcl does not have operators as part of the language syntax, but it provides an "expr" standard command which evaluates strings as expressions which share all arithmetic, bit-wise, relational and short-circuit operators with C. There is no support for "++" or "--", but the "incr" command can often be used as a substitute. Tcl adds "**" for numeric exponentation, "eq"/"ne" for explicit string matching, and "in"/"ni" for list containment testing.
* Julia provides basically the same operators as C, including "==", "=" and those of the "+="-kind, but it uses other symbols for some of the operators. For instance, "$" is used for XOR, and "^" is used for exponentiation. Most operators are the same, though. Some additional operators are "\" is reverse division (exchanged operand order). Some operators like ">=" are the same as in C but have additional UNICODE aliases using the actually correct mathematical symbols. Comparison operators are chainable, i. e. lower <= middle == other_same_middle <= higher. Julia also has a vector-variant of most operators by prefixing them with a dot. For instance, "a .+ b" applies the "+" operator to every pair of elements of a and b. This only makes a difference if a and b actually have elements, i. e. are some kind of container type like a vector or matrix.
* Wren provides most of C's basic operators except those which make no sense in a scripting language (such as the adress-of operator). In exchange, it adds range operators: "1 .. 3" would enumerate 1, 2, 3 and "1 ... 3" would emumerate 1, 2 when interating over the range. "3 .. 1" is also allowed and would enumerate 3, 2, 1. However, Wren also omits all decrement/increment operators as well as all the combined assignment operators. That is, it only has plain "=", but no "+=" etc. Which means you actually need to write "i = i + 1" in Wren; there is no shorter way.

Modules, namespaces and scoping of identifiers.
All languages provide some means for the developer to assign identifying names to things. Most importantly this will be variable names, but user-defined functions, subroutines and methods will also often be associated with identifiers. However, the question arises whether the same identifier can be used for different things at the same time. Some languages allow to extend identifiers with module or namespace qualifiers, making the qualified name unique event though the identifier itself isn't. Yet another possibility is to restrict the scope where an identifier is visible, thus allowing to re-use the same identifier in different scopes. The language may also allow shadowing existing identifiers from outer scopes by overriding them with local identifiers in nested scopes.
* C++ provides namespaces as a general means of grouping definitions of all kinds together under a given namespace qualifier. In addition to that, classes, structs and unions represent namespaces themselves. The only problem with C++ namespaces are preprocessor macros which do not care about namespaces in any way.
* One should know about records that it is not permitted to have two records defined in the same OCaml module which both have an entry with the same name. This is quite unfortunate, and considered a pretty stupid design bug of OCaml.
* Objective-C does not provide namespaces and global objects are usually given a small namespace-like prefix in order to avoid name clashes. However, due to the usually short size of those prefixes, the effect of this custom is limited.
* Ada provides a namespace concept very similar to C++ (actually, it might rather be the other way around). Except that the term "package" is used rather than "namespace". Like C++, Ada allows to create, populate, locally rename or import namespaces.
* Tcl has namespaces, and they are much more important there than in other languages. Specifically, namespaces are also used to create instance variables for objects, and may be mis-used to store bound variables for emulating closures. Both techniques exploit the fact that in Tcl, a namespace essentially nothing more than a slot in an anonymous global hash, where the slots can be indexed by the namespace name. Namespaces can also be nested. Instance variables for objects can therefore be created by using some global counter to create a new namespace (containing the counter value) for every object, and then place the instance variables for the object in that namespace. The object itself is frequently returned as the name of a object-specific command which executes code or invokes named methods in the context of the associated namespace.
* Rust allows both shadowing names by declaring them again in the same scope, and shadowing them by declaring them in nested scopes within the same function. However, a variable is only destroyed when it leaves its scope, not just because it becomes shadowed within the same scope. In other words, shadowing a variable within the same scope cannot re-use the memory allocated to the shadowed variable, even though it will not be accessible any more until the end of the scope.
* Nim supports block scope just like C, which may some might see as a huge advantage over Python which only has function scope. Functions bodies, "for"-, "while"- and "block"-statements create a nested scope.
* Variables in Wren have block scope. There are only variables, no constants. However, static read-only getters can be defined for classes, which act much like constants. Variables must be initializes when they are defined. Instance variables (one leading underscore) and class variables (two leading underscores) are different, though: They can be used without being declared. It is not even necessary to assign to them first (in this case they return the null value when read).
* Namespaces in Wren are rather strange. I could not even manage to access a different class which was declared in parallel in the same source file. Neither seems nesting of classes to be possible, except for nesting them within a function/method within a class. The idea seems to be that every class must be put into a different source file than another class using it, because then it is possible to use the "import" keyword for that. It seems only possible to directly access classes and functions nested within the current namespace, but none from surrounding namespaces.

Late or early binding.
Binding is the process of associating symbolic names with actual code (or data) references, especially in the context of library functions.
Early binding is usually done when compiling source code, while late binding is done at runtime. Late binding is clearly more flexible, allowing even to change the binding between different instances of binding. The downside is that the compiler usually cannot report binding errors at compile time. Such errors can then only detected at runtime. This is especially dangerous in error handling code, because such code is not normally triggered, and thus the error cannot easily be detected in advance. Even worse, if the error is actually triggered, it will lead to an exception within exception processing, which can lose the original error message in some languages.
* C and C++ have early binding (except for shared libraries - which are not a language feature, though). Although pointers and dynamic casting can still lead to runtime errors, the chance of missing members at runtime only is typically small for a "class" or "struct".
* Objective-C: Non-object-oriented function calls are exactly as in C. But all method invocations are resolved completely dynamical. The target objects to which messages shall be sent are specified as a static symbol reference which will be resolved to a function address by the run-time. Caching is supported. This means extremely late binding; message handler addresses are only resolved when they are actually invoked for the first time. Objective-C is not based on v-tables but rather on message handler functions. Each object implementation exposes exactly one function for resolving message names to method handler addresses. Whether this function uses vtables, Hashes or other means of actual symbol resolution of message names is a decision left to the implementor. This method has also the potential to defeat the code-bloat problem of all vtable-based languages like C++: In the object implementations, all declared v-table entries must be implemented, even though only a few of them might be actually used. And all methods invoked by those unused methods must be implemented as well. This leads to a cascade of dependencies, where a large number of methods are part of the memory image of the executable even though most of it might never be actually called. Objective-C avoids this cascade of dependencies; the implementor only needs to deploy dynamically-linked libraries containing code which is actually used. There is no need to include libraries only because other libraries (which are required) also contain unused methods which would require them.
* Python, Perl: Late binding. If you make a typo when writing the name of an attribute, you won't detect it before runtime (and even then only if you are lucky).
* Ruby: Also late binding, but at least defined accessor functions must be used to access instance variables. The accessor definition duplicates the identifier, so there might be a chance by the just-in-time compiler to detect mis-spelled method names (if the compiler is that smart).
* In Erlang late or early binding can be chosen by meta-instructions. The default is late binding, i. e. Modules are only loaded when required.
* In Tcl, code is generally always passed a strings. It therefore depends only on the specific control flow command used when it actually chooses to execute the code or not. This decision also concerns variable bindings. However, because all references in Tcl are symbolic, bindings are usually "late", because they refer to the name of the symbol at the time it is resolved by executing the code. The interpreter makes some optimizations however when arrays or namespaces are indexed with literal indexes, but this behaviour is transparent and should not effect observable behaviour (except possibly for execution speed).

Conditional compilation
This only applies to compiled languages. Sometimes it is possible to evaluate constant expressions at compile time and include or exclude sections of code to be compiled based on the test results.
* C/C++ provides the "#if"-preprocessor directive for that purpuose. While not strictly part of the C language itself, it de facto is, although evaluation of the expression is limited to information available to the preprocessor. However, the decision which code section to include is normally made based on defined macros, and the preprocessor has access to those.
* Nim provides the "when" Statement for condition compilation. Syntax-wise it is nearly identical to Nim's if/elif/else statement, except that all expressions will be evaluated at compile time and will include or exclude arms of the "when" into/from compilation as a result.

Demand linkage.
Interpreted languages typically include some sort of "modules" by an explicit statement, and those modules (typically source files) contain definitions of not only the required functionality, but all the functionality in the module. Compiled languages with linker support typically allow compilation-unit-level- or function-level-linking which only extract those parts of the imported modules which are actually referenced and used in the application. Apart from the initial overhead (which might be substancial for languages such as C++), compiled languages with demand-linkage might have less in-memory code bloat than interpreted languages.
* C++: The linkage editor allows to extract specific member functions from a library compilation unit. This is called function-level linking. Inline functions are handled by the compiler and are not subject to the linkage editor. Functions which are not member or inline functions are treated like C functions regarding linkage.
* C: The linkage editor extracts any referenced compilation unit from the library archives. Apart from non-standard extensions, this is done in an all-or-nothing fashion. For this reason, a C compilation unit (for a library module) should only contain such definitions which will always be required together anyway, such as constructor- and destructor-like functions for the same object. Any optional member-function-like definitions should be placed in separate compilation units.
* Objective-C uses the same mechanism as C and C++ for referencing non-object-oriented symbols from other translation units. Contrary to C++, however, method invocations are not handled or even seen by the linkage editor at all. Method names are just symbols (such as a hash of the method signature or the address of some internal static variable specifically generated for the message by the compiler) to be resolved at runtime. Therefore, demand linkage in Objective-C is normally only used for statically-linked utility functions that do not represent object-oriented method handlers. This approach has the advantage that it completely de-couples method invocations from method implementations. Methods defined in the API need not be implemented at all if they are not actually required; not even as stubs. Also, message-sending code never needs to be recompiled because of internal implementation changes in the target object. Forwarding and delegating messages is also made easy. The disadvantage is somewhat higher runtime overhead for each method invocation and the fact that automatic demand-linkage is not possible related to object invocations. It is thereby the duty of the programmer to ensure that all required objects will actually be available at runtime. On the other hand, an application will not crash because of missing method handlers. Instead, an exception will be thrown or the message will be ignored, depending on compiler options and the choices made by the implementor of the object processing the message.
* JAVA creates .jar files containing sets of compiled classes. But this is essentially very similar to a shared library. There is no automatic process (I am aware of) which puts only those .class files which will actually be required by an application.) While this behaviour is OK for .jar files which are intended to be used by many applications, it is code bloat .jar files which are actually being used as mere "convenience libraries".
* Perl supports the autosplit/autoloader module, which allows to split library module sources into individual functions, which can then be imported individually, thus saving all the other (potentially unused) functions and objects defined there. Unfortunately, there is no tool (I am aware of) which removes the unreferenced files left of this split-down operation, so it is a mere runtime optimization, but will not avoid code-bloat when deploying applications. (Although, admittedly, some smart deployment manager might try to remove unreferenced file by itself. But if one considers the potential complexity of Perl source files, this might be a daunting task.)
* Ada allows to put packages into separate files and compile them separately. Overly long packages can even be split among multiply files as subunits, but they remain a single package from a conceptual point of view. Packages can also be split into a declarative part similar to a C header file and a body which contains the actual implementation. The declarative part can further be subdivided into a publicly visible part and a private part. Ada has its own make utility which will compile units only on demand due to changed dependencies or if the package's source text has changed.
* Tcl provides commands to load additional Tcl code from files, which can be organized as logical "modules". It also provides some framework support for this, such as for locating the module, creating indexes or fast-load stubs for delayed command loading, defining symbols to be exported or not, etc. But all this can also be done by the programmer himself by simply loading strings containing Tcl scripts from somewhere. Tcl's module support is therefore highly extensible if required, but looks rather primitive by default (read: it does not do much by itself, so much the work like updating index files is delegated to the library module designer).
* Go supports demand-linkage at least similar to C (with translation unit granularity). It is unclear whether it also provides function/variable-level linking like C++ does.

Strings and character sets:
* JAVA strings are immutable, hashed entities. There is a class StringBuilder which is a mutable buffer which can be used to construct strings before they are actually created. All strings are 16-bit UNICODE only.
* Lua strings are immutable, hashed entities similar to JAVA's. Lua does not associate any particular character encoding or collation sequences to strings natively. In Lua, strings are 8-bit-clean and provide strict byte semantics. However, if the operating system provides support for the locale features of the ANSI C runtime library, Lua can make use of that support and many of its standard string functions will honor the current locale then. Lua only stores a single instance of the contents of each string, and if different string variables have the same content, they will all share the same instance. This makes creation of new strings moderately inefficient, because each new string must be hashed and entered into a global internal hash table. On the other hand, it makes string comparison (for equality) very efficient. Lua therefore uses string constants where other language might use enumeration constants or symbolic constants, because comparison is similarly efficient, and multiple copies of a string constant do not incur any additional storage overhead due to string instance sharing. Strings in Lua have value semantics, not reference semantics. This makes operations like string concatenation rather inefficient, but special programming techniques can be used to ameliorate the adverse effects resulting from this. For instance, there is a standard library function for contatenating all the strings in a table into a new string. Using this function is much more efficient than appending one string after the other, each time creating a larger intermediate string. This function, combined with a special string-stack algorithm related to the "Towers Of Hanoi" game, can concatenate even a large number of rather big strings with only moderate storage overhead and without excessive garbage collection of intermediate strings.
* Perl strings are fully mutable. Internally, they are not, but Perl cleverly and transparently handles string modification which gives the programmer every freedom. I can tell from my experience that dealing with UNICODE is a constant source of trouble in Perl. Better avoid it if you can. Perl loves simple byte semantics, and altough UNICODE is supported in most situations, it always feels like a second-class citizen.
* Python's support for UNICODE is cleaner integrated into the language than Perl's, but at least in the times of Python 2.2 or so, I had experienced extremely negative impressions when trying to use Python's UNICODE in a Win32 Console window.
* C strings are primitive, null-terminated arrays of type "char", which is an integral type containing a single character's ordinal code. There is a special type for wide-characters (and strings based on it), but the language does not specify *how* wide actually. In most implementations, wide characters match either the requirements for UCS-2 (Windows) or UCS-4 (Linux) characters.
* C++ strings are rather sophisticated objects provided by the standard library, but the built-in-strings are mostly the same as in "C".
* In Erlang strings are just normal lists of integers, where each integer represents a byte of the string representation. Erlang has no special built-in support for UNICODE, locales or collation sequences. However, libraries might be used to provide equivalent functionality. Using the built-in list representation, there cannot be done much more with strings than concatenate, input, output or compare them for binary equality. The recursively constructed list representation can also lead to O(N^2) effort for certain string operations unless care is taken and the linked list structure is considered when implementing a string function.
* PROLOG treats strings in the same way as Erlang, i. e. singly-linked lists of byte values with no special consideration regarding character endoding let alone UNICODE support.
* Haskell also treats strings as singly-linked lists of single characters, but here the characters are at least represented as arbitraty UNICODE code points. This does not make string processing more efficient than in PROLOG or Erlang, but at least there is no problem in storing arbitrary UNICODE strings.
* OCaml does not know anything about Unicode; it uses ISO-8859-X to encode characters and strings. There are several third-party libraries that provide support for Unicode. You will need to use these (or implement your own) if you want to work with anything but ISO-8859-X. The strings themselves are implemented as built-in data types; they are mutable and can be accessed efficiently by index. Which means, OCaml obviously does not use that brain-dead linked-list-representation of character strings as PROLOG, Erlang or Haskell.
* Limbo supports UNICODE natively in UTF-8 encoding. However, only the BMP subset of UNICODE is supported.
* PHP has no real UNICODE support.
* Ada provides 8-, 16- and 32-bit characters, and arrays of such characters as strings. String variables and constants can be initialized either with string literals, or with array literals containing character elements. The character types are actually built-in enumeration types with character literals as enumeration values.
* Ceylon stores strings as UTF-32. The source code is expected to be ASCII, but command line options can be used to explicitly define a different source character encoding.
* Tcl seems to have full UNICODE support, but this support does not extend beyond the scope of "characters". That is, there is special no support for "user-perceived characters", which are units of base characters and combining characters. I am not even sure whether Tcl supports characters outside the basic multilingual plane yet, but I think so.
* Julia supports the full range of UNICODE characters and UTF-8, UTF-16 and UTF-32 encodings. By default, string literals are always UTF-8 (or from its ASCII subset). It has a "Char" type similar to C, which can store a single 32-bit codepoint. Its "String" types are immutable, just like in JAVA. Strings are constructed by combining other strings (or "Char"s), such as concatenating them. Strings are indexed as arrays by 1-based byte positions rather than character positions, which allows them to be stored efficiently internally rather than as UTF-32 or similar, and access is still fast. Array slices like in Python are supported as well, and the special symbol "end" automatically represents the last string index position. Julia supports the same escape sequences as C and C++ does, including '\0', '\e', UNICODE hexadecimal escapes. String literals can be enclosed in double or triple-quotes, but the latter have useful special behavior for blockwise indented text in multiple lines. Julia also supports "interpolation" of variables into strings borrowing Perl's "$"-Syntax. The basic syntax interpolates only simple variables, but by enclosing expressions within round parentheses complex expressions can be interpolated, too. String literals prefixed by "r" are treated as regular expressions of the PCRE-library, which is also available in Julia. Strings prefixed by "b" are "Byte" strings, which are similar to normal Strings but use "UInt8" rather than "Char" as their base types. Strings prefixed by "v" are version number literals, which behave like to be expected when compared (such as for sorting).
* Go has immutable strings and provides keywords (rather than just normal functions) for concatenation, comparison as well as character set conversion to and from UTF-8. Strings consist of 8-bit-bytes which need not necessarily be UTF-8 encoded, although by convention they usually are.
* Wren assumes and supports UTF-8 encoding of strings, but actually provides byte-string semantics in many situations. There are also functions for conversion between UNICODE codepoints and UTF-8 bytes. One unfortunate decision has been that the "count"-property of strings assumes UTF-8 encoded strings and returns the number of code points rather than the number of bytes. This is unfortunate because the search and substring access functions use byte offsets instead. It is slower, too. Therefore, the string has to be converted into a byte array first, and only then its "count" property gives the byte size. Wren supports "interpolation" within its string literals using the "%(expression)"-syntax. Interpolations can also be nested within interpolation expression, although even Wren's author itself disencourages using this feature. Literal "%" needs to be escaped with a backslash, otherwise Wren requires the interpolation syntax and raises a compile time error. In Wren, interpolation is a compile-time feature for string literals only. It cannot be applied somehow to dynamically created format strings (except via the eval() function). The upside to this restriction is that interpolation has zero runtime overhead in comparison to functionally identical code that avoids interpolation. In fact, Wren has solved the notorious performance problems of the printf()-family of other languages using a very simple solution. By restricting the interpolated expressions to comprehensible variable names, the resulting strings can even be considered "translation friendly" for localization.
* Normal Scala string literals do not provide interpolation, but string literals prefixed by "s" do. Both "$simple_name" and "${expression}" are supported.

Built-In-Functions:
* Perl provides the usual built-in functions for dealing with strings, lists and other types of built-in types. Perl was even a trend-setter in more than one case, for instance it's idea to propagate the result value of a boolean shortcut operator. It also has the "do"-Operator, which basically allows to include normal statements like "if", "while" etc. somewhere inside an expression as a part of it. This allows arbitrary nesting of expressions and statement sequences in extremely powerful ways, especially when combined with Perl's functional list processing primitives like "map", "grep", etc.
* Erlang has the usual arithmetic and boolean operators, but no strictly built-in functions. However, there are in fact built-in functions which are part of the "erlang"-Module, which is automatically imported into each user-written module. Which means the functions of the "erlang"-module can be considered to be built-in functions, and most of them can also be accessed as if they were local functions (that is, there is no requirement to qualify them with a module name when using them). The implementation might also choose to actually implement some of the "erlang"-functions as real built-in functions.
* In Ada there seem to be no built-in functions except for the usual arithmetic operators. Everything else can be found in the packages of the standard library, just like in C. On the other hand, there are "pragma"- and "for"..."use" directives which could be interpreted as some kind of builtin function. Pragmas control the compiler and "for"..."use" assign or override special attributes of data types, packages, subprograms and other language constructs at compile time. Many types have also built-in attributes such as the declared starting and ending index of arrays, but these are typically queried by the program and either constant or dynamically changing.
* Tcl: Too many to be listed here. Actually, everything in Tcl is a "command", and functions are just commands which return some value. There is no syntactic or conceptual difference between built-in or user-defined commands. The language code has dozens if not hundreds built-in commands, and many of those commands are actually command groups, supporting subcommands. Therefore, one of the hard things when learning Tcl is learning those many standard commands. This is a similar problem like in LISP.
* Wren has no built-in functions at the global scope, but rather provides built-in methods such as "count" for its built-in types such as "List" and "String". There is a standard library consisting of several predefined modules, and the module handle acts as a namespace for the provided functions. Only very few functions are provided which interact with the OS, far fewer than in Lua. On the other hand, there is more built-in support for functional programming and other things that will generally be useful independent of the I/O-mechanisms and OS. Even string support is very basic and only contains primtive search- and substring functions, but does not make the attempt to provide more powerful pattern processing like Lua's built-in regular-expression. I think this was a good decision.

Attribute syntax for function calls:
Some languages prefer attribute syntax like "string".length() over function syntax like length("string"). Although many languages allow both syntaxes depending on the user's preference, some enforce attribute syntax for built-in functions and native data types. In those languages, it is necessary to write something like 42.to_string(), 1.iterate_to(10), "text".mid(3, 1) and so on. In my opinion, this is a mis-use of attribute syntax which should only set or get attributes which can be naively be expected to be available within an object. This means "string".length may be justified, but 1.iterate_to(10) certainly wouldn't be. Also, I think that length("string") or "string".length is more appropriate than "string".length(), because attributes should be conceived as values, not functions, unless unavoidable.
* Ceylon loves attribute syntax and expressions like " ".join(list). It is not everywhere that bad, but attribute syntax is clearly more often used than necessary.
* Wren allows to define getters which will then use the "object.property" syntax (without parentheses). In contrast, "object.method()" is a method-call without arguments.

Data Attributes Of Objects:
Some languages allow using data attributes in an instance object using a similar syntax as accessing normal variables. Other languages require a class to provide methods for getting and setting data attributes using the usual method invocation syntax. Even other languages actually *use* normal variables as data attributes.
* Being a pure object-oriented language, Ruby does not allow direct access to data members inside object instances. Instead, accessor methods must be defined for that purpose. However, those accessors are invoked by normal assignment syntax or component-access syntax by the object's client. There are also shortcut-declarators which define accessor functions for reading or writing simple data attributes, so programmer don't have to do that manually.
* C++ data attributes are normal variables without any "intelligence" of their own. The only way to restrict access to them is by restricting visibility ("private" and "proteced") or declare them as "const". There is no way to add access restriction logic as code. Therefore, accessor functions to data attributes are usually defined manually, and all data attributes are defined inaccessible from outside. But this is pure convention; nothing stops badly designed classes from actually exposing instance variables as externally accessible data attributes.
* Perl works the same as C++ but even lacks the capability of restricting the visibility of instance variables. Therefore, any class client can examine and modify the full state of any instance object at any time. Like in C++, accessor functions are usually defined manually, but there too is no language-enforced oligation to actually use them.
* Python basically has the same weak support for data attributes as Perl has, but it provides one extra feature: If an attribute has a name starting with two underscores, it will be expanded automatically into a longer form containing the name of the class; thus lessening the chance for name clashes and emulating "private symbols". Those symbols are not really private, though; they just got assigned a "mangled" name. However, for practical application such mangled names are a good enough approximation of private names.
* In Ada, objects are essentially records, and so there is no problem directly accessing data fields of such a record. Accessor functions may be defined for accessing private variables, but those are normal subprograms and no special syntax is provided.
* In Ceylon, object members may be accessed directly using dot syntax if they are declared as "shared" in the class.
* Wren allows data attributes of objects, but they can only be accessed by methods and even then only for the current object. It is not even allowed to access the attributes of a different object of the same class. Access to data attributes is otherwise only possible via setters and getters.

Getters and Setters.
Some languages provide a means to automatically invoke setter and getter methods of objects when the source text looks as if just an ordinary object field value has been read or written. Behind the scenes, however, such languages translate the field assignment into the call of a setter or getter method of the object, which may transform the values to be stored or even synthesized values to be returned from thin air. This often also enables the programmer to disallow read or write operations, either by raising runtime errors or  by just leaving out an implementation for the undesired access operations. Languages without such support require the client of the object to directly call the setter or getter routines, creating a visual distinction between "dumb plain value fields" of an object or values to be set or read via some accessor function.
* C/C++ does not have special language support for getter/setter functions. It is possible, however, to overload certain operators such as () or "->" for partially achieving such an effect.
* Lua provides metatable-methods which can be used to implement automatic setter- or getter-methods on tables. Since tables can syntactically be used either as arrays, hash tables or like C "structs", this provides everything needed to redirect simple assignments to accessor functions. Reading or writing can be restricted by raising runtime errors in the accessor functions.
* Ada has no support for implicit getter/setter invocations. Normal subprograms have to be used.
* C# and Vala have a sepecial syntax to declare getter/setter methods for object properties. The compiler will then automatically transform seemingly normal struct field assignments to method calls for those accessor functions. Properties can be read-only, write-only or allow both reading and writing.
* Ceylon allows to define getters and setters for retrieving and setting attributes through user-defined methods. Interestingly, there seems only to be the choice to implement either getter, getter and setter, or none of both. In particular, implementing only a setter (for write-only attributes) does not seem to be possible. Likewise, it is unclear to me whether implementing a getter but no setter makes an attribute read-only or not.
* Rust has no special syntax for getters or setters. One has to write then as normal methods manually. Even worse, Rust does not have named arguments, variable argument lists, function overloading or constructors either. Instead, the "builder pattern" is used to emulate constructors: Instead of possibly multiple constructors for the same target object, a specific "builder" type is created instead of every constructor. Every builder exposes a "new" method which populates the instance variables of the builder with default values. Then the builder implements a setter for every possible "constructor argument" which updates the associated instance variable of the builder object. The setter also returns a reference to the builder object, making the setter calls "chainable". As the last method in the chain a "finalize"-method is called, which uses the current state of the builder's instance variables to create the actual target object. This pattern is obviously powerful, but it creates a lot of work for the implementor if a constructor has many optional arguments. Also, a lot of function call may be involved at run time. On the other hand, hopefully most constructors won't provide many optional arguments. So far for the recommendation and theory. In practice, several Rust code examples I saw which needed to create objects did not care to use that tedious pattern. Instead, they initialized all instance variables (which were contained in a simple struct) directly using with a struct initializer expression. Obviously, such code will refuse to compile if more instance variables should be added to the struct definition later.
* Wren requires getters and setters for any access to the object's instance variables. Getters are defined as functions without argument lists, while setters are defined by appending a "=" to the attribute name and using this as a method name. The instance variables themselves need to start with a single underscore. For an instance variable "_x", the most basic getter would be "x() {_x}" and a basic setter would be "x=(value) {_x= value}".

Iterators:
Iterators represent a unified means in a programming language allowing to enumerate all the items of a sequence.
* Python uses the "for" statement for iterating over a sequence. The object argument to "for" can be any object providing an __iter__() function returning an enumerator object. That object must have a method next() returning the next object in the enumeration or raising a "StopIteration" exception when done. Alternatively, functions can be used as iterator objects. In that case, they must use "yield" to return their results.
* Go specifies a special "range"-variety of the "for"-loop which allows to iterate the elements of a hash or array. It can enumerate either only the keys/indexes, only the values, or both.
* Ada has a "for" statement which can iterate over ranges. Ranges are always characterized by a starting index and an ending index. Container objects like arrays usually provide a "Range"-attribute which can be used with the "for"-Statement.
* Ceylon defines an interface "Iterable" which other classes can implement in order to be used as iterators. The "for"-statement works only with iterators. "for" allows to iterate either only over values, or over both indices and associated values. It seems not to be possible to iterate over indices only, although it is of course possible to just not use the iterated value. Ceylon distinguishes between iterator types which can produce no output and ones that must produce at least one element. Syntactic sugar is provided for creating implicit iterators on the fly by using operators like "..".
* Tcl provides a "foreach"-command which can iterate over lists. Most (all?) Tcl container types allow to represent themselves in the same syntax as a list when converted to strings. Therefore, "foreach" can be used for iterating most container types. However, for this to be efficient, it is important that the application does not require constant conversion between string representation and internal representation of a container, a problem called "shimmering", or although things will quickly become very slow. This is only a problem for large collections, however, as the Tcl interpreter is pretty fast for what it is doing.
* Wren allows every object to support iteration by implementing two methods: iterate(currentIter) returns the next/updated iterator object and iteratorValue(currentIter) returns the value associated with the current iterator object.
* Nim has two kinds of iterators: "inline iterators" which are more or less what most other languages would call a "generator". Such iterators may contain loops and can "yield" the iterated values, but they are severely restricted in that recursion is forbidden. They are essentially macros which will be expanded by the compiler as part of the loop in order to provide their values. The other kind of iterators are "closure iterators". Those have mostly the same restrictions although a few less, but will not be inlined and are implemented as some special kind of functions.

Normal and Multi-Precision Arithmetic:
Some languages restrict the value ranges allowed in numerical calculations to some hardware-dependent limits. Multi-precision arithmetic, in contrast, allows to perform numerical calculations with virtually arbitrary precision at the cost of speed and memory.
* Python has a long integer type which can store integers of arbitrary size without loss of precision.
* Ruby has different internal integer types for machine-sized integers and arbitrarily-sized integers, but conversion between both types is handled transparently for the code using it. This combines the advantages of both types into a single pattern of usage.
* Perl does not have built-in multi-precision arithmetic, but external modules can overload arithmetic operators to provide it. Besides MP integer arithmetic, there is also a module for floating-point MP or even rational MP arithmetic. However, the overhead of Perl's MP arithmetic is relatively large.
* C/C++ do not have any built-in support for MP arithmetic but have to use external helper functions or object classes to provide that functionality. Like Perl, C++ implementations can use operator overloading in order to implement that.
* FORTH has only double-size integers in addition to the normal integers and floating point numbers. There is no support for MP-arithmetic in a standard implementation.
* Limbo has only 4 native types: 8 bit unsigned; 16, 32 and 64 bit signed. This means there is no support for 32 or 64 bit unsigned. This is a very unfortunate choice for many cryptographic and other bit-parallel algorithms. It also makes primitives for library-based implementations of multi-precision arithmetic less efficient. And as Limbo generates code for a portable VM rather than native code, it is unlikely that these limits are about to be expanded ever.
* Go has built-in multiprecision arithmetic, but only for intermediate results as it seems. There is no actual variable type for such values and the values will be truncated on assignment depending on the target type.
* Ada has a clean type hierarchy of built-in types which includes integral, fixed-point and floating point types. It even subdivides Integers into generic integers, positive or "natural" numbers. However, all those types seem to be based in platform-dependent native types, so there are very few guarantees concerning their actual value ranges. There is no automatic support for multi-precision arithmetic that can be taken for granted. (But of course, library functions may provide such support.) On the other hand, Ada has the quite unique capability to let the programmer define required value ranges instead of requesting specific types. For instance, a variable can be defined for storing integers from 1 to 100. The compiler will then automatically pick the most appropriate native type. In a similar way the number of significant digits can be defined for a floating point variable of the required resolution for a fixed-point variable. Normal integer arithmetic in Ada will detect and disallow arithmetic overflow, but specific "modular" integer types are available to emulate C's behavour (only for unsigned integers, though).
* The built-in arithmetic of Tcl works similar to Python and automatically converts integer values which exceed the range for internal native integers into arbitrary precision "wide integers". This is only done for integer values. Floating point values are restricted to their internal precision and may even incur an additional loss of significance due to frequent conversions between text and internal representations which are not always lossless.
* Julia supports only the following native numeric types: Signed and unsigned integer types with widths of 8, 16, 32, 64 and 128 bit. Floating-Point-Types with widths of 16, 32 and 64 bit and full support for NaNs, signed zeroes and infinities. Based on those, rational and complex types are also natively available. Rationals are automatically reduced to lowest terms. Julia also automatically wraps (no special import/loading necessary) the GMP and MPFR libraries to provide the BigInt and BigFloat type constructors for arbitrary precision arithmetic. Which means they can be used pretty much like the built-in native types, except that the constructor has to be used for creating literals. Julia has automatic type promotion for numeric literals which ensures that always a fitting type will be used for them. But this does not extend to calculations, where explicit type conversions are necessary or the results may overflow, which is not considered to be an error. Such overflow, which can only occur with the built-in fixed size integer types, is even exactly defined. A bit odd is the fact that the only way to directly specify an unsigned integer literal is via hexadecimal notation. That is, "0xa" is unsigned while "10" is signed. (But one can always use an explicit conversion like "UInt(10)" for coercing a specific type.)
* Wren only supports the arithmetic of C/C++ "double", plus temporary 32-bit integer arithmetic for bit-parallel operation. However, as all relevant operators can be overloaded, it should not be too hard to custom-provide a class for multi-precision (or any other kind of) arithmetic.

Cascading Condition tests:
Often the value of an expression has to be tested against various criteria in order to determine how it should be processed.
* C/C++ has a normal if/else construct which must actually be nested to achieve "else if"-functionality. If also has a "switch"-Statement which compares a given value to a list of comparison values and resumes control after a matching comparison value. "switch" can only compare simple integral values by equality, and the comparison values must be constant and known at compile-time.
* Python has a if/elif/else construct, but no "switch"-construct like C/C++.
* Perl is similar to Python, but anonymous hashes which map a list of comparison values to a list of anonymous inline function definitions can be used to easily emulate C's switch() statement functionality.
* FORTH has no built-in support for this; logic operators work bitwise as in BASIC. However, due to the extensibility of the language, it should be possible to create custom implementations of this although an unusual syntax for this might be required.
* Rust has an "alt" statement instead of a "switch" statement. If can do the same things except the "fall-through"-behaviour, and also allows "|" ("or") and "to" (for numeric ranges) within the match expressions - and not just constants to be compared for equality. Update 2017-08: Is this still true? Rust provides a "match" expression which can do pattern matching and also provide the same capabilities as C's "switch" except for its weirder aspects like the fallthrough semantics. Rust's "match" will also check whether all possible cases have been covered and will give an error message otherwise. match can not just compare values, it can compare types and destructure composite values as well. The special value "_" can be used as a "catch-all" pattern. When used in place of a variable name for binding a placeholder within a pattern, the catch-all pattern means to ignore the associated parameter value. While the catch-all pattern only matches a single field, ".." does the same for all remaining fields (e. g. within a tuple). Logical "or" is possible within the same pattern with "|". Value ranges can be matched with "...". In value matches, the comparison value in the pattern can also be bound to a name, so its literal value does not need to be repeated if used in the resulting expression. While destructuring, the extracted fields can be renamed. Partial destructuring (only extracting certain fields of interest) is supported as well. Matches on structure can be combined with value matches using "match guards", which are a special form of "if"-clause which can be added after the end of a pattern. Those guards can already make use of any extracted fields which have been destructured by the associated pattern.
* Go has a "switch"-statement like C has, but by default it jumps out at the end of each "case" to the end of the switch construct, unless a "fallthrough" keyword is present which makes it behave like "C". In Go, "switch" is actually a more fancy way to write an if/else-cascade using the same comparison value, so the order of the "cases" is significant - and this behavior is well-defined. As a consequence, the comparison value are not restricted to constants; they can also involve variables and generally be arbitrary expressions yielding a comparison value. The "switch" test expression can be omitted, in which case the "cases" must be normal condition cases as for "if"-statements. This acts as a replacement for C's "if"/"else if"-cascades which are not possible in Go because if and else always require parenthesized body blocks.
* Ada provides a conventional if/then/elsif/else statement as well as a "case"-Statement like C's "switch". But unlike "switch", there is no "falling through" (and therefore no need for a "break" before the next "case" like in C). It is possible to check for multiple values with "|", similar to "case" in the UNIX shell. The "case" in Ada is a bit unconventional in that it does not allow the test expression to not match any case at all. Therefore, either must be matching clauses specified for all possible values of the conditional expression, or a default clause ("when others") must be provided. For instance, when the conditional expression is a boolean, the compiler requires that both a "true" and "false" are checked for, or only one of those plus a generic check for other values.
* Ceylon has a switch-statement like JAVA, but eliminates the classic "fall-through"-behaviour of C.
* Tcl has a "switch" command which which can do the same as C's except for the braindead "fall through"-idiosyncracy. Also, Tcl's "switch" is much more powerful, supporting not only exact matches, but also - optionally and only for the command as a whole - case insensitive matching of strings, UNIX "glob"-style matching, and even regular expression matching. It is further possible to re-use a case body for multiple adjacent cases, i. e. perform a logical "or" between different case patterns. A "default" clause is also supported, but it can only be used the last case expression. Finally, case patterns are not restricted to constants, but can also include variables which will be substituted internally by "case".
* Wren does not provide "case" or similar, not even "elif". One has to use normal "if"/"else" cascades. The only other option is to dispatch handler code via container types such as hashes / "Map"s.
* Nim as a "case"-Statement for comparing a candidate value. Each action clause test can compare the candidate against a list of ranges of values (and of course also against lists of simple values or against just a single value). There is no "break"-crazyness as in C, only a single clause is processed.  "else:" can be used to specify a default case. Without "else:" Nim ensures that one of the cases is actually taken, there is no "silent fall-through" for all tests. However, this only applies to ordinal types where the compiler knows all possible values, like integers or set types. Strings can also be compared, but then there will be no warning if none of the tests matched. Also, ranges are not allowed for strings - just single values or lists of them.

Forking And Re-joining Control Flow Paths:
Sometimes control flow has to be diverted from the normal path and re-joined at some later point, typically skipping some code. For instance, a nested loop might have to be left prematurely, resuming the next iteration of an outer loop. Or control flow has to leave several nested loops prematurely, continuing after all nested loops.
* C/C++ have only support for leaving or skipping an iteration of the innermost loop with the "break" and "continue" statements. For more complex cases a "goto"-Statement is available which can jump to any defined label within the same function. Non-local jumps can be emulated by throwing an exception or using the longjmp() library function. A "return"-statement allows to leave the currently executing function at any time. The "exit"-function can be used to leave the whole program at any time.
* Perl provides the same capabilities as C/C++, but extends the direct support for loop control diversion to multiple nested loops by introducing "loop labels". Also, Perl's "goto" Statement is much more powerful, it can even divert control directly to some label within a completely different function, cleaning up and establishing new stack-frames as required. Furthermore, Perl's "goto" can accept an expression which evaluates (at run-time) to a suitable descriptor for the destination for the jump.
* Python provides "break", "continue" and "return" as C/C++ does, but not "goto". Therefore, exceptions must be used for diverting flow-control when multiple loops have to be left prematurely. As Python iterators throw StopIteration exceptions for similar purposes, the performance overhead incurred by exception processing can be expected to be within reasonable bounds. Python also features an "else" clause for several flow-control statements which is used when flow-control left its construct "the normal way" rather than prematurely.
* FORTH has the usual loop control constructs similar in power to those of C. There is no direct support for leaving multiple constructs directly, however. Exceptions or condition codes may be used to emulate this. FORTH also has no "goto". However, FORTH allows the user to define any custom control structures which are possible on a stack-based architecture.
* Ada has an EXIT statement which can prematurely exit a loop. Loops can optionally have names, and in such cases EXIT can also specify which of the loops to leave. Interestingly enough, Ada only provides a "while" loop and an endless "loop", but no "repeat" ... "until" (or "do" ... "while") loop. Such loops need to be emulated with an endless "loop" containing a conditional "exit" at the end of its body. In addition, RETURN and exceptions can be used to leave a subprogram at any point.
* Ceylon has C-like control structures, but always requires curly braces around blocks of conditional code even if a block happens to be a single statement. This is necessary to eliminate ambiguity in the language syntax which uses curly braces for other things as well. Ceylon's "for" loop only works for iterators and features an optional "else"-part for leaving the loop normally rather than with a "return" or "break". "while" is like in JAVA.
* Tcl does not have a goto command, but its "return" command can be used in a exception-like way in order to leave nested control structures (or even function invocations) in a "exception"-like controlled manner. "break" and "continue" are also available and based on the same underlying mechanisms. Unfortunately, this means that "return", "break" and "continue" are rather expensive statements in Tcl because of the power they provide. Returning from a function or prematurely exiting any loop has the same basic cost as throwing an exception (without an error message or course). On the other hand, this seems the only way to ensure control flow changes across control flow command boundaries, because control-flow commands can be user-provided and need not necessarily be built-in.
* Rust has both "break" and "continue" which only exit or restart the innermost loop. However, it also supports "loop labels" using a special identifier syntax. Unlike generic labels loop labels can be put only before the opening keyword of some type of loop, and after "break" and "continue".
* Wren has only the "break" and "return" statements of C. There are no labels, "continue", or any kind of "goto". Which means code will become complicated when you have to break out of more than one level of loop. Frequent usage of ugly PASCAL-style "flag" variables is to be expected. On the other hand, if Wren's "fibers" can be used to emulate exceptions, it can be expected that they could be used to "break out" of nested loops, too.
* Nim has "break" and "continue" which by default refer to the innermost "while", "for" or "block" construct. The latter one can have an optional label, and in this case the "break" and "continue" can use this label for referring to a different than the innermost construct.

Concurrency and SMP support:
Some languages provide multithreading on an interpreter-level only, often called "green threads". That is, although it is possible to create multiple seemingly simultaneous flows of operation, there is actually only a single flow of operation multiplexed by the interpreter or language runtime into what seems to be different threads of execution. In essence, it is time-based non-preemptive scheduling under complete control of the language runtime. In contrast to this, real concurrency is created when different physical processor cores actually execute different paths of execution at the very same time. Some languages support neither form of concurrency, some support interpreter-level concurrency only, and some support both.
* OCaml bytecode and native code programs can be written in a multithreaded style, with preemptive context switching. However, because the garbage collector is not designed for concurrency, symmetric multiprocessing is not supported. OCaml threads in the same process execute by time sharing only. [http://alan.petitepomme.net/cwn/2002.11.26.html] In summary: there is no SMP support in OCaml, and it is very very unlikely that there will ever be. If you're into parallelism, better investigate message-passing interfaces. [http://en.wikipedia.org/wiki/Message_Passing_Interface#OCaml] The OCamlMPI Module implements a large subset of MPI functions and is in active use in scientific computing. To get a sense of its maturity: it was reported on caml-list that an eleven thousand line OCaml program was "MPI-ified", using the module, with an additional 500 lines of code and slight restructuring and has run with excellent results on up to 170 nodes in a supercomputer.
* PHP: No.
* Rust provides green threads called "tasks". Tasks communicate with one another using "ports" and "channels".
* Ada has full support for "tasks", a kind of generalized platform-independent multi-threading and associated synchronization primitives.
* There is a Tcl command for creating additional interpreters, which may possible run in parallel. I'm not sure. However, SMP is nothing Tcl is very concerned in particular. It is rather the contrary, as Tcl makes heavy use of global variables. All commands (except for lambdas implemented with "apply") are referenced by global names, and Tcl code frequently dynamically creates additional commands at runtime, also referenced by global names. Regarding this, multi-threading in Tcl would be hard, while multi-processing is certainly possible. The fact that Tcl has no garbage collection might make multi-processing even similarly effective as in C, as more data can be shared between processes (as long as the code does not modify it after forking).
* Go supports threads via a built-in construct called "goroutines". Every function or method call can be prefixed with the keyword "go", executing them concurrently. Current implementations seem to map goroutines to OS threads and sub-schedule them at this level. A big benefit of this is that the application does not need to be concerned with thread creation (including hairy details like specifying a stack size) and destruction as this is automatically accomplished by the Go runtime. Communication between goroutines is normally implemented by message-passing through so-called "channels", another Go built-in mechanism. If a function returning a result is invoked as a goroutine, the return value is lost. However, a similar effect can always be achieved by writing the result to a channel instead of returning it, and have the caller receive the result from there. goroutines are not automatically safe against race conditions, but the following simple convention usually solves this: Shared references should be treated immutable by the receiving goroutine, and only objects for which the ownership (the only remaining external reference) is passed to the goroutine may be modified by the latter. This idiom is called "aliases xor mutable".
* Wren does not support any of those, at least not as part of its core libraries. It only provides its "fibers", which are non-preemptive and scheduled by the interpreter itself.

Running speed:
Some languages are interpreted, others compile into bytecode, and the remaining ones compile into native code. The latter is usually the fastest, but also creates the largest executables and native code can only run at the hardware platform it has been generated for. Bytecode typically runs on every platform, but some implementations create platform-specific bytecode. Bytecode is typically more compact than native code, which means bytecode executables tend to be smaller than native executables. Interpreted-only languages, usually referred to as "scripting languages", have the advantage that no compiler/linker is required for development. The code can be executed by the interpreter as soon has it has been written. There are also hybrid solutions, such as compilers which create temporary bytecode or native code im memory only, and execute it immediately instead of writing it back to the disk.
* FORTRAN compiles to native code. It can be very well optimized by the compiler and typically outperforms all compiled languages in terms of speed. Other reasony why it is so fast is because (by default) FORTRAN-programs do not have a stack or heap to be managed, but the compiled code only uses a fixed set of variables in fixed memory locations.
* C / C++ always compile to native code and generated typically the fastest executables except for FORTRAN. C and C++ can use preprocessor macros in order to avoid the runtime overhead for actual function invocations, but only C++ allows to define functions as "inline" which has quite a similar effect as a macro, but avoids the problems inherent with preprocessor-based source text substitution such as lack of namespace support, potential multiple evaluations of arguments etc.
* Objective-C is like C except for method invocations. Those are not implemented as direct function calls or via a vtable, but require a two-step protocol: First, the message name has to be resolved into a handler address. Second, this resolved address has to be used as a function pointer for actually calling the function. However, one a message name has been resolved by the runtime, the function pointer can be cached and re-used by later invocations. Popular Objective-C implementations claim to have a total overhead no larger than a factor of 3 for the first invocation of a method compared to a native C function call. Further invocations using the cached address are claimed to have no larger overhead than 50 %. Function inlining as in C++ is not supported by Objective-C.
* JAVA generates bytecode which is in turn interpreted by a virtual machine. Several VM implementations use Just-In-Time compilation when executing this VM code, which translates frequently used portions of bytecode into a more efficient type of byte code or even into native code. JAVA can thus have excellent performance when running under such VM implementations, but of course it will never beat C's or FORTRAN's native code generation.
* Perl compiles its input scripts into an internal bytecode representation which is then executed. There are also some efforts to create native code. However, Perl's performance is generally very poor if only the speed of control flow is examined. But Perl has a large number of useful builtin functionality which executes very fast and is used very often by Perl programs. This normally gives Perl scripts a decent or even good overall performance, although the actual statement throughput is rather embarassingly. (For instance, good old MS-DOS QBASIC outperforms Perl when implementing the Sieve of Erathostenes.) Perl has typically good to excellent performance when it comes to text processing.
* FORTH is usually among the fastest threaded code interpreting languages, but it will not beat natively compiled languages. There are commercial versions of FORTH available which feature native code generation. Generally, FORTH creates quite efficient straightforward code but lacks all the clever optimizations which optimizing compilers can do. An advantage of FORTH is its integrated Assembler which allows to implement runtime-critical words using native assembler code.
* OCaml has a reputation for providing very efficient native code compilers; a compiled OCaml program will typically run at 50 % of the speed of a compiled C program performing the same task. This is generally considered to be a very good factor for a garbage-collected functional-style language. OCaml programs therefore are considered among the fastest executables, superceded by FORTRAN, C and C++ only.
* Ada is available as separate compilers, but also as a front end which uses the normal GCC als the code generator backend. Most of Ada's run-time checks can be disabled, resulting in a speed not too different from C. However, the different design philophies of the run-time-libraries would suggest that Ada applications will most likely run somewhat slower than C, but not by much. Part of this might be due to the fact that Ada always checks for exceptions in library functions, while C programmes frequently choose to ignore error checking.
* Although Ceylon compiles into JAVA VM bytecode and should therefore be expected to run rather fast, the language suffers from the fact that no "primitive" data types are available. In contrary to JAVA, in Ceylon really *every* value is an object. This means "Integer" must be used where "int" could be used in JAVA. This makes the language much slower than pure JAVA (especially in inner loops), and also creates a larger memory footprint.
* Tcl can be expected to be slow as a dog, with C running circles around it. However, it should still be an order of magnitude faster than using normal shell scripts for labor-intensive tasks. Also, the fact that many Tcl data structures can be present either as strings or as some optimized internal form, Tcl ist not *necessary* very slow. However, there is a problem with references from within a command to variables living in global scope or a namespace. Explicit references are possible but the code would be hard to read because of the long names, so usually local aliases are created in the local scope using "upvar", "upvalue", "namespace eval" or "my". Unfortunately, all those commands are executed every time a command is called; they are not just declaration handled by the compiler somehow. This negatively effects especially the speed of object-oriented code, as the aliases for all instance variables which need to be used have to be created again and again for every single command invocation.
* As a language intended for number crunching, Julia is relatively fast. It runs about half the speed of optimized natively compiled C code. On the other hand, this is the same thing many people say about JAVA. Maybe the fact that both languages use JIT-compilation makes them run at a similar speed, even though Julia JIT-compiles directly to native memory while JAVA code runs inside the JVM. However, traditional compiling is not the only way Julia can be used. It is also possible to compile a Julia program only temporarily into RAM, similar to many other languages like AWK, but creating native code like a JIT compiler does. Julia calls this "just-ahead-of-time". This leads to noticable larger startup overhead for such programs, but avoids the requirement to install any machine-specific native code for an application prior to execution.
* Wren is supposed to be faster than standard Lua and about half as fast as Lua-JIT. In other words, it is pretty fast. (Assuming one believes the author who states this and provides the benchmark results.)

Comparative Statements:
* OCaml seems to me like a gentle introduction to Haskell. It has some of the same features, but also includes support for more familiar programming paradigms (i.e., imperative and OO). Also, OCaml seems to knock the socks off of just about any other language other than C/C++ when it comes to performance comparisons  a very good thing since much of my school work/research makes use of some extremely time consuming code.
* One thing that hasn't been touched by others: in Haskell it's hard to program graph algorithms, due to its lack of support for mutable arrays. Yes, there are work-arounds (either pass state around using monads, creating a new array for each assignment; or use a GHC extension), but I found the alternatives unpleasant.
* Haskell has a nicer syntax than OCaml. But being a purely functional language, sometimes more efficient iterative solutions to a problem cannot be directly implemented. Haskell is much more slower than OCaml, however the differences diminish the less non-functional features of OCaml are used.
* Aus meiner Sicht gibt es einfach keinen Grund, Groovy einzusetzen. Scala macht eigentlich alles besser als Groovy und ist noch dazu deutlich performanter und dank statischer Typisierung, knnen Entwicklungsumgebungen potenziell auch mehr untersttzen. Auerdem ist Scala als Sprache einfach ausgefeilter. Achja, und eine
interaktive Konsole gibt es bei Scala auch. Wozu also noch Groovy nehmen? http://www.heise.de/developer/news/foren/S-Wer-braucht-Groovy-wenn-es-Scala-gibt/forum-199127/msg-20175444/read/
* Ada will catch 90 % of the errors in applications which will not be found by a C compiler.

Outstanding Features:
Some languages provide certain features which are not commonly found in other languages. Those are usually nice features, but some might be just strange.
* Erlang supports hot code replacement. This means code modules from a running programm can be updated without stopping the program, providing some conventions for structuring the code are followed. This allows Erlang programs to be bug-fixed or extended in place without a need to terminate and restart the application. This might be a killer feature in a heavily productive system where downtimes are not acceptible.
* Objective-C defeats the code-bloat problem induced by all vtable-based approaches. Methods need not be implemented just because they are defined as part of some interface which are in use by the application. They need only to be present when actually used. When application components are packaged as DLLs, only those DLLs need to be deployed which contain implementation of methods actually used by the flow of control in the application. Should unimplemented methods be invoked accidentally, an exception will be thrown, allowing the application to handle this situation in a controlled way.
* Rust has a "!" declarator which allows a function to be declared as "no-return". It is aiming at binary  compatibility with C, which means it is possible to define compatible data structures. Furthermore, Rust is intended to be capable of being used as a system programming language with a high degree of interoperability with C.
* Vala is a JAVA/C# like language that is actually just a precompiler for C. Interoperability with C is therefore quite high, while it has a much more comforting syntax and features. It combines the ease of JAVA/C# programming with the power of C programming. It is much easier to learn than C++, yet provides many of its features. In contrary to most other JAVA/C# like languages its dynamic object management is *not* garbage-collected, but rather based on reference counting. As Vala is just a C Precompiler, the generated code can be compiled on all plaforms providing a standard-compliant C-compiler, as long as it is possible to port/compile glib for that platform too. Unfortunately, Vala only inherits C#'s syntax, not its security features. Specifically, Vala cannot automatically instrument code in order to detect buffer overflows like JAVA can. It is therefore much more readable than C or C++, but not really more secure. It will be more secure than C in practice, however, because it provides exceptions which help to avoid "forgotten" error checks which are a common problem in C.
* Although Ada's syntax clearly belongs to the PACSCAL family of languages, it is quite unique in the extent it allows to catch both runtime and compile-time errors. If one needs to choose a language which provides both good performance and maximum support for avoiding programming errors, Ada is a much better choice than C. It is als quite appropriate for highly complex and large software system. In situations where security and reliability is more important than raw speed, Ada should be favored. The caveat is that it is a large and complex language and not easy to learn.
* Ceylon has union and intersection types and a fully static type system. It also features control flow instructions to "narrow down" types to subtypes, allowing runtime-dispatching on types without resorting to casts and weakening the type system. Null-pointer exceptions cannot occur in Ceylon because of this, at least unless native JAVA code is invoked which is still possible.
* Tcl is one of the few scripting languages which uses reference counted data structures rather than garbage collection. This means that Tcl, at least in theory, should use less memory to run, and avoid unprecictable "pauses" during memory allocation operations, at the expense of running somewhat slower in geneneral because of permanently updating the reference counts. But considering the overhead the language has anyway due to its interpretative nature and heavy dependence on strings to represent "everything", the small overhead incured by reference counting is probably totally neglectible. Tcl is likely a good choice for launching many scripts as separate processes in parallel on the same machine, as the memory requirements per interpreter instance will be much less than in garbage collected languages. Another thing to mention is Tcl's capability to launch functionally reduced sub-interpreters for executing untrusted scripts from an external source.
* Perl features "tainted" variables. When running the Perl interpreter in a special tainting mode, any input to the script is considered "tainted" and must be converted into untainted data before being allowed as output or inclusion in a result. Tainting propagates in expression evaluation, much like calculations with NaNs in floating point operations. Only a very restricted set of means is available for "untainting" values, generally meaning that all input data must be fully parsed in order to become untainted. This feature is intended to avoid things like SQL injection attacks.
* Standard-FORTH implementations provide the "SEE" word, which can "disassemble" an already-compiled word for which the source-text is no longer available. This will not always work perfectly because "immediate" words can work like macros during compilation so only the result of their doings is visible with SEE.
* FORTH is more then a programming language, it is normally used as an interactive system which can even act as an operating system on embedded systems.
* Standard-FORTH implementations provide a word for testing whether a BLOCK word set is available, and if it is then the words of this set can be used to read and write the attached mass storage devices in units of "blocks". Blocks are units of 1024 bytes, and the concatenated bulk of the storage device's native blocks will be re-interpreted and subdivided into chunks of this size. For instance, every two adjacent 512-byte disk sectors may become a 1024-byte block. There are no files, no directories, no fileystem - the FORTH programs can read and write "blocks" specified by their block index. That's all.
* FORTH implementations providing "blocks" also provide in-memory buffers for storing "blocks" and words to manage them. This can be seen as a sort of manually-managed "disk cache", allowing the actual "block" words to directly access the storage media without additional cache layers.
* Standard-FORTH implementations which provide the BLOCK word set usually also use blocks for storing text, in particular FORTH source code. When FORTH's 1024 byte "blocks" are used to store such text, they are referred to as "screens" where the 1024 bytes will be interpreted as 16 lines of 64 ASCII text characters each. There are also words available for editing such screens, including full-screen editors.
* When "screens" are used to store FORTH source code, there are words for reading and executing FORTH commands from a screen with a particular index number. If the source code is comprised of more than one such "screen", the last line of that screen will contain a FORTH command for reading the next screen. This allows arbitrary chaining of screens, but the user is burdened with keeping track what block indices are used for storing which data.
* Nim only has a single built-in range iterator, "..". It always includes both ends in the iteration. However, if the upper value is prefixed with "<", then it will be excluded from the range. Thus "0..<10" will enumerate the valued 0 through 9.
* Nim allows to use statements like "if", "for" etc and even variable declaration within an expression, if that expression is wrapped within parentheses and semicolons are present to separate the statements. The last statement must be an expression which will also be the result of the whole parenthesized expression. This feature can be quite useful for calculating the initialization value of a run-time constant (variable that becomes read-only after initialization) run-time constant. If the statements produce a constant result, this construct can even be used to initialize a compile-time constant.
* Nim's formal parameters of a function are call-by-value and read-only. If is possible however to shadow formal parameters by declaring new variables with the same name in the function, and use the original value of the parameter in the initialization expression of the new variable. It is possible to declare formal parameters as "var", which will then use call-by-reference and the function can change the caller's value. This feature has obviously been borrowed from PASCAL.

Source text formatting:
Some programming languages do not care at all about how the source code is formatted. Few enforce a particular formatting style. Most establish some convention which are recommended for being used, especially in larger projects with more than a single source code contributor.
Indentation is frequently used as a means of structuring the contents of source files.
Indentation uses either specific indentation characters or generic whitespace.
"Whitespace" refers to invisible control characters like "Newline", "Space" or "Horizontal Tabulation". Even though those characters are invisible in the sense that there are no visible glyphs, they nevertheless affect the visual appearance of a source text by occupying space on the display.
In languages like FORTRAN whitespace is mostly ignored, while in other languages like Python whitespace is very important. Virtually all languages treat a comment as if it were all whitespace, and whitespace within quoted string literals is preserved. This will therefore be assumed in the remainder of this text section unless stated otherwise.
* C/C++ are mostly free format languages; any amount of whitespace can be inserted between two tokens. Exceptions are preprocessor directives which must always be the first tokens on a new line. The block structure of code is indicated by curly braces; otherwise each statement or expression is considered a separate block.
* Python uses indentation to idicate the block structure of the source text. The style guide "PEP-8" recommends to indent by units of 4 column widths. However, for some strange reason, PEP-8 is not included with the local documentation and thus requires Internet access to be obtained.
* Perl works mostly like C. However, it adds special syntactic constructs like "formats" and "documentation sections" which have different semantics for whitespaces (especially regarding the newline sequence).
* Haskell has a syntax which basically uses whitespace only as separating tokens, where whitespace is purely optional in many situations. However, there is a feature called "layouts", which allows indentation to be transformed into syntax elements such as semicola or enclosing an indented blocks by curly braces. There are some rules governing the co-existence of layout-style and normal indentation-agnostic style within the same source file, but generally they can be mixed quite freely in the ways that make sense to the reader.
* Ada is a traditional programming language, and therefore whitespaces has no effect except in comments. Ada only features "--"-comments (like in SQL and Lua) which extend to the end of the line, but not further. There are no comment blocks, and inorrect nesting is therefore also not possible.
* Tcl uses semicolons or newlines as command terminators. The special backslash-newline escape can be used to convert a newline with all adjacent whitespace to be transformed into a single space character. Also, actual parameter values can be quoted and are then allowed to contain multiline-strings. Command substitution also allows multiple source lines within a single parameter. Beginners find it hard to learn when list() needs to be called in order to add an extra level of quoting to an argument list, or when this is not necessary. This can induce dangerous bugs, because code might work as long as the arguments do not contain whitespace, but will fail it there is.
* Rust is a free format language with arbitrary amounts of whitespace separating the tokens. However, the Rust documentation states that "Rust style" indents by 4 spaces, and explicitly not by a tab. A source formatter named "rustfmt" has been under development with the intention to make it part of the standard Rust toolchain eventually.
* Wren normally allows only a single expression per line. Expressions can span multiple lines as long as the newline is put in a position where the line does not represent a complete valid expression or statement. Within a line, whitespace separates tokens. There are no rules regarding indentation enforced by the language.
* Nim uses ASCII space for indentation of structural code blocks. Tabulation characters are forbidden for indentation. The amount of indentation is not dictated by the language: The compiler only differentiates between lines of equal, deeper or less deep indentation for recognizing structural code blocks.

Arguments and Lists:
When a function or procedure is called or defined, the list of arguments must be specified somehow. Also, some languages support named or optional arguments and even completely variable-sized argument lists. Optional arguments often allow (or even require) default values to be specified. Similarly, most language support a syntax for specifying list literals, for instance as array initializers or enumeration type definitions. Some arguments provide named parameters which can be used by the caller in order to pass arguments in arbitrary order.
* C has fixed-sized argument lists. The arguments are separated by comma (","). As comma also serves as an operator, it will only be considered an operator in argument lists if the expression is put within parentheses. Arguments are associated by position only; there is no support for named arguments. Optional arguments are partially supported in the form of variable argument lists. However, there is no way to determine the size of a variable argument list unless this information is known in advance or can be derived by examining the other (non-optional) arguments. The whole argument list is always enclosed within parantheses. Array initializers or enumeration types are denoted using a similar syntax, but curly braces are used instead of parentheses.
* C++ argument lists are the same as C's, except that optional arguments are also supported. Optional arguments must always specify a default value; if an actual parameter is missing the default value will be substituted for it. Optional arguments are only allowed at the end of a argument list. To be more specific: No mandatory arguments can follow an option argument. Only other optional arguments may follow an optional argument.
* In Perl, real lists are passed as argument lists. Unless special syntax is used, all argument lists are variable in size and have no predefined structure. In contrary to C, the actual size of the argument list can always be determined. Argument list items are separated using the comma like in C, but there is no requirement for enclosing the argument list within parentheses (except where necessary due to operator precedence). Named arguments are not directly supported by the language, but it is possible to interpret the whole argument list (or any trailing part of it) as a set of key/value pairs by initializing an associative array with it. This is, however, runtime behavior: The language itself considers an argument list to be a list and nothing else. Nevertheless, interpreting parts of the argument list as named key/value pairs is a very common convention in Perl, especially for functions which take a large number of arguments.
* In OCaml, actual functions always have exactly one single argument and currying is used to emulate the effect of multiple arguments. Nevertheless, the compiler provides syntactic sugar for argument lists with multiple arguments. In this (very common) case the arguments are only separated by whitespace; i. e. there are no commas in argument lists. This lack of argument separators can make it hard to identify the arguments of a function call if whitespace is also used within the expressions which make up the arguments. However, due to operator precedence, non-trivial arguments must typically be put between parentheses anyway, and those same parentheses will also help to visually distinguish the argument expressions.
* In Lua, functions can have any number of arguments, and the formal parameters might be more or less than the number of actually provided parameters. Superflous actual parameters will normally be ignored, where missing formal parameters will be set to the special value 'nil'. However, ellipses can be specified as the last explicit formal parameter, which allows any number of additional actual parameters to follow. These parameters can then be represented within the function as an expression also denoted as epllipses, which evaluates to the list of additional arguments. This list can either be used as initialization values for an array, or on the right side of a multiple-assignment expression. Any arguments passed to a Lua script from the outside can be accessed via the global table "arg" which will be set up by the regular Lua shell or Lua script interpreter.
* Ada allows to use a mixture of positional and named parameters in subprogram (function or procedure) calls. It is up to the caller whether or not to use named actual parameters for the callee's formal parameters. In any case, positional arguments must precede all named arguments in a subprogram call. Default values can be defined in the formal parameter list. Quite unusually, Ada allows not only to rename subprograms by assigning new alias names for them, but also allows such renamed subprograms to specify different parameter names and default values than the original formal parameter ist. (Formal parameter types and -modes have to be the same, though.)
* Tcl has basically fixed arguments for command like C, but supports anonymous variable arguments at the end of the argument using the special formal parameter name "args" (which is special only in that context). In addition, the fixed argument can define default values, much like in C++ (but with a different syntax). And like in C, there is no way for the caller of a function to refer to its arguments by formal parameters names, and therefore there is also no way to re-order the arguments in doing so.
* Rust supports array value literals and not just as initializers. It also supports "tuples", which are heterogenous lists of fixed size, and can be used to "wrap" multiple values into a single value. Among other uses, this can be used to return multiple values as the result of a function, which only allows one return value directly. Tuples can be deconstructed into their constituent values by the syntactic pattern-matching features of the language. Contrary to some other languages, Rust's tuples can be assigned with other (compatible) tuples, and the tuple elements can even be indexed with an array-indexing-like (but different) syntax. Rust does not support normal function arguments to be referenced by name by the caller. However, struct values require initialization by name could be designed to be used as arguments instead. Rust does not have variable-sized argument lists either. Instead, objects like vectors may be passed as single arguments, which can contain a variable number of values.
* Wren has just plain argument lists. No fancy things like packing or unpacking argument lists, variable argument lists, or argument re-ordering by using names when calling a function/method. Variable arguments can be emulated to some extent by passing literals of heterogeneous "List"s, and named arguments can be approximated by passing literals of "Map"s.
* Julia has variable argument lists. They are implemented by appending an ellipses token (three dots) to the last formal parameter of a function. Then this argument will receive a tuple of all the remaining arguments provided by the caller. Arguments can optionally be named, in this case the formal argument must also provide a default value. This default value will not be evaluated if the caller actually provides the argument, however, so there is no overhead involved with that.
* Scheme allows to pass parameters normally, i. e. by order, or by keyword name if a keyword was defined for a parameter. The last two arguments can be a dotted pair, which makes the right last argument a arbitrary-sized list of the remaining parameters.
* Nim's formal parameters are declared pretty much like in PASCAL: Comma-separated sublists of names followed by a colon and then their shared type. Any number of such sublists are allowed as the formal parameter list, separated by semicolons. The return type of the function is also declared using a comma. Somewhat uncommon, the body of the function is introduced with an "=", and the body itself follows indented in the lines after that. Nim supports both default values and named arguments. Making use of the latter by the caller is optional.

Function return values
Many languages can return at most one value from a function. Some can return any number of values directly. Other allow returning a "tuple" of values which can automatically be "unpacked" or "deconstructed" into multiple result variables. Some languages provide am explicit "return" statement which can return a value. Other languages always return the value of the last evaluated expression. Many languages allow to ignore the value returned from a function, while some will complain about this. Several languages differentiate between prodecures which do not return a value and actual functions which always do. Between those positions, several other languages return a special pseudo-value often called "void" or "unit" in order for functions to signal that they actually do not want to return anything at all.
* C/C++ use "void" to declare functions which do not return a result. Some compilers warn if return values are ignored by the caller, but most don't.
* Every FORTRAN function has an automatically defined variable with the same name as the function itself. Assigning to this variable sets the function's return type.
* Nim allows a function to omit its return type declaration. Such functions are then procedures and cannot return a value. Only a single value can be returned by function that declare a return type, but any number of "var" formal parameters can be declared by the function for returning values by reference. Nim will complain if provided return values are ignored by the caller, unless the special "discard" statement is used by the caller which has been explicitly crafted for this purpose (besides others).
* Although Nim has a "return"-Statement which can return a value like many languages, it also provides an automatically defined variable with the same name "return" which will be initialized with the default value of the function's return type (usually 0 or nil) and will be returned automatically if there are no return values or if they do not specify a return statement at all. Despite of its special purpose, this variable can otherwise be normally used within the function, such as adding values to it within a loop.

Arrays:
Arrays are data structures with constant-time read/write access to individual elements, but inserting or removing elements from somewhere in the middle of the array requires an effort which can be calculated as a function of the index value and the current number of elements in the array. Array elements are identified by incrementing integer indices. The starting index for arrays is either user-defined or predefined by the language. Arrays normally cannot have "holes", although they might contain special values indicating that a specific array slot does not currently contain anything useful. Arrays have dynamic or fixed lengths. Fixed-size arrays have always constant access time for elements, but growing or shrinking dynamically sized arrays may impose significant runtime overhead depending on the implementation. Fixed-sized arrays are normally adequate data structures for real-time applications. Some languages do not provide arrays as described above, but rather emulate a similar behavior using hash tables. However, hash tables have different timing characteristics than real arrays.
* Perl has arrays as well as hashes. Arrays always start at index 0. Individual array elements can have different types. Arrays can grow dynamically.
* Lua does not provide arrays as distinct language feature, but uses them internally as a mere optimization when a "table" is created which is indexed with incremental integers starting at 1. When indexes other than integers are used as indexes, or if the integer indices used contain "holes", the array will be converted into a hash table transparently and automatically. Individual array elements can have different types.
* C/C++/OCaml: Arrays are provided by the language and always start at index 0. Their size is fixed. Arrays can only contain elements of the same type, although that type can be a polymorpic type (a union or a template type). In any case, all array elements have the same size. It is a common practice to just store pointers to the actual elements in the array, which always have the same size no matter of the size of the data they are pointing to.
* Awk always emulates arrays by hashes.
* Go does not support normal arrays but rather more powerful slices. A slice is a starting pointer into an allocated buffer with a specific length. It is possible to extract sub-slices out of slices. Sliced start out by dynamically allocating a buffer which returns a slice representing the whole buffer. There is no pointer arithmetic in Go, but subslices provide a similar effect, except there is no danger of buffer overflows. Slices do not only have a length, but also a capacity which represents the actually allocated number of elements within the slice. The slice length can be set to an arbitrary value between 0 and the slice's capacity, emulating the effect of a growing or shrinking array. An slice can never exceed its capacity, though. This scheme is very powerful because it allows to preallocate space for dynamically growing arrays for inspace-usage rather than creating a lot of garbage when the contents of smaller arrays have to be repeatedly copied over to slightly larger arrays.
* Ada has multidimensional arrays. Array subtypes can be defined which do no specify dimensions, but even in those cases the language ensured that the run-time will know the actual dimensions of the array, and can therefore check whether the used indexes are valid. Arrays index ranges can be specified explicitly (negative and positive range boundaries are supported), implicitly by providing initialization values, or both. Initialization values can be provided with implied indices in ascending order, but explicit index values can also be specified for better readability. Character arrays can be initialized with string literals. Array slices are supported and can also be used to perform batch assignments of array elements. The concatenation operator defined for strings also works for arrays and can be used for slice concatenation in batch assignments. Arrays, like most other Ada objects, have built-in attributes which can be used by the programmer. In the case of arrays, those include the first and last index value as well as the number of array elements. The only specialty here is that in an empty array, the end index is one less than the starting index. In particular, an empty string constant translates into a character array with a starting index of 1 and an ending index of 0, containing 0 elements. It is not necessary to provide literal initial values for each element of array initializers. Specifying the special keyword "others" as an explicit array index, the assigned value will be used repeatedly for all the remaining array elements not already initialized.
* Tcl features one-dimensional "arrays" which are however actually associative data structures, allowing arbitrary strings as index. Multi-dimendional arrays can therefore be emulated by concatenating the different array indices with separator characters. However, Tcl does have "lists", which are numerically indexed. Unfortunately, "lists" have the disadvantage that the can only be passed to a command (or returned from it) by value, and not by reference. Therefore, the only way to modify particular list elements, is passing the whole list into the command, and returning a modified new list from it. Neither is it possible to pass list elements as arguments to commands in such a way that the called command can modify the elements without getting the whole remaining list as well. This can makes things inefficient, and therefore Tcl "arrays" (read: hashes) are frequently used even if they are indexed just by contiguous integers which would make a "list" the more appropriate date structure, except for the beforementioned restrictions.
* Rust arrays are fixed-size, 0-based and always 1-dimensional (i. e. "vectors"). Of course, they can be nested, allowing to simulate the effect of multi-dimensional arrays. Array values (such as for initialization) can be created by enumerating a comma-separated list of elements within square brackets. There is a special syntax for creating an array value of a particular size with all elements set to the same scalar value. Rust also supports array slices. Slice-based, dynamically allocated growable arrays are available through the standard library as a generic type "Vec<T>". Both array and Vec elements must be indexed by a particular type "usize"; arbitrary integer types are not automatically coerced into that type. This is a good thing because it does not hide the actual runtime overhead of necessary type conversions from the developers, making them aware of it, and nudging them to use the right indexing types from the start. Array accesses are bound-checked, at compile-time (if possible) and also at runtime. (I am not sure whether this can be turned off.)
* In JAVA, native arrays are always allocated on the GC-controlled heap and have a fixed size and member type which must be specified when they are created. The declaration of array variables includes their element type but not the number of elements. This is separate from array creation where the number of elements (which does not need to be a compile-time constant) has to be specified. However, both can be combined by adding an array initializer to the declaration. In this case, the number of elements in the initializer will determine the array size. Arrays have a ".length"-property providing their element count. The standard library provides additional non-native array-like types which can grow or shrink dynamically.
* JAVA's native arrays are always 1-dimensional, but the syntax allows arrays of arrays to be accessed as if they were multi-dimensional arrays.
* JAVA provides special optimized functions for array operations (copy, search, compare, fill, sort) such as System.arraycopy() or from the java.util.Arrays class. Those should be used where possible instead of doing things "manually".
* Wren has "list"s which are in fact 0-based arrays with non-negative integer offsets. "Sequence"s and "Range"s behave like lists in many situations, but are actually different things. The "count"-property of lists specifies their current size. Lists are mutable. Lists can only be enlarged by using the add()-method. Wren uses square brackets for both indexing arrays and providing list initializer literals.
* Ruby supports array objects which can be created by array initializer expressions. Array initializers are a list of comma-separated expressions enclosed in square brackets. Array-accesses look like a[i] where i is a 0-based numeric value (which will be coerced into an integer) but not a hash key type.
* Julia's arrays are normally 1-based, and this extends generally to most container object indexing also, such as to strings. Regarding strings, Julia uses byte offsets rather than character offsets, although using a variable-length encoding (UTF-8). But it only allows actual access at the starting bytes in an initial shift state, otherwise an exception is thrown. This allows both efficient access and correct UNICODE handling using support functions for determining allowed byte offsets in a string.
* Crystal has integer-indexed 0-based native arrays. However, it is easy to implement array-like objects which can be used as drop-in replacements for native arrays but will use a completely different indexing scheme. Like in Perl, negative indexes can alternatively be used to index arrays from the end. For instance, a[0] is the first element and a[-1] is the last element of array a.

Runtime bound checks:
Some languages can detect buffer overflow errors, while others do not care. Among the languages which support such checks, most allow the checks to be disabled for performance reasons.
* PASCAL, Ada and JAVA provide bound checks for arrays. At least for PASCAL and Ada, those can be disabled for performance. I am not sure about JAVA in this regard.
* C, C++, Vala: No checks are performed. Tools like valgrind can partially compensate for that, but they are usually only installed on developer machines and therefore not available "in the field". C++ and Vala allow to divert array-accesses to accessor functions which might be used for bound checking, but this is strictly an application issue and not provided by the language itself.
* Python: Those languages always check array bounds; there is no way to switch them off. As a consequence, security leaks as a result of buffer overflows are impossible for code written in those languages. Unfortunately, the underlying runtime and operating system kernel is written in C most of the time, which renders the combined system still potentially vulnerable to buffer-overflow exploits. However, the chances are vastly diminished compared to other languages.
* Perl, awk, Lua: Those languages check array-bounds internally, but create the illusion of infinite arrays when accessing them. When trying to assign to a non-existent element, that element is automatically created. When trying to read the value of a non-existing element, some special value is returned that can be checked by the application. Another way to describe this is that those languages will extend an array (or hash) to include the incorrect index/key, rather than complaining about it. Invalid memory accesses cannot happen as a consequence, but index errors may go undetected because checking the returned special values for missing elements is optional and not enforced.
* Tcl has no pointers or directly-accessible buffers, and therefore classical buffer overflow exploits are not possible. Attempts to read non-existing elements of associative arrays also generate runtime errors.
* Julia does bound checks on arrays and strings. Invalid accesses trigger exceptions. However. the bounds checks can be disabled by a command-line option for additional speed.
* As Go uses slices rather than simple pointer arithmetic for accessing arrays and dynamically-allocated buffers, runtime bound checks could be performed because the slices have known allocated and reserved sizes. Whether this information will be actually exploited depends on the implementation and compiler settings, but one can assume it will normally be done for security benefits.
* Wren checks the current number of elements in lists before allowing to index them. Lists can only grow by calling the add() method on them or concatenating existing lists.

Arithmetic overflow/exception checks:
Some languages raise a runtime error if the value of a variable exceeds its maximum as the result of an arithmetic operation. Others just clip the result so that it can fit in the variable, producing an arithmetically incorrect result.
* C, C++ do not care about arithmetic overflow. Most platforms use modulo 2^N interval arithmetic, but the C standard does not guarantee this. Similarly, the effect of a division by zero and other undefined arithmetic operations produce undefined effects and must be avoided in portable code. But this is easier said than done, because in most situations there is no efficient way how to determine whether the arguments of an operation would overflow before actually executing the operation.
* PASCAL, MODULA and Ada can check for arithmetic overflows. It is possible to disable those features, though, for performance reasons.
* Python automatically switches to multi-precision-arithmetic once the result of an arithmetic operation exceeds the range representable by a native machine word. It tries to uses native machine words otherwise. The programmer does not need to care about this; it is all done transparently behind the scenes.
* Common LISP defines an "arithmetic tower" which seems to be somewhat like Python's approach, but I think it is still possible to enforce C's method explicitly for performance reasons. As such, common LISP seems to combine the best of both worlds: Reliability by default, and speed at the cost of reliability where it is desired.
* In Tcl, numeric overflows are impossible, because integer calculations automatically use arbitrary precision arithmetic when necessary, and floating point calculations support infinity as a built-in safeguard against overflows.
* Wren uses C's "double"-arithmetic, and therefore overflow and underflow can never occur and will be represented by infinities when necessary.

Hash tables:
Hash tables are data structures much like arrays in that they associate an element with some kind of index value. However, the index values of hash tables need not be integer values; they can virtually be of any type of value. There is no ordering relation defined between hash indexes. It must only be possible to compare them for equality. Integer indexes can be used like in arrays, but those indexes say nothing about the actual order of internal address locations used for the elements within the data structure. Hash tables can therefore be used to simulate sparse arrays; i. e. with holes in them. In contrary to arrays, reading, writing, insertion and deletion of hash table entries are constant-time operations on the average, but actually depend on the size of the hash index key used as well as on the current internal state of the hash tree. This gives near but not quite constant-size access to hash table elements most of the time. Depending on the algorithms used, the hash table might need internal reorganization as it grows or shrinks, which might impose significant runtime overhead for the access which triggers it. Because of this, most hash table implementations might not be adequate data structures for real-time applications.
* Perl's hashes can grow dynamically and use a highly sophisticated hash-function which yields good internal index distribution with arbitrary key types. The hash is function also parametrized by an internal random-derived key value in order to make key-order related attacks on implementations more difficult. This also helps find incorrect assumptions made by the application about the order in which hash keys will be enumerated without sorting them first. Perl hashes can contain individual elements of different types.
* Go's hashes are accessed mostly like arrays, except for a different declaration syntax and support for non-adjacent index values of different types. Accessing non-existent hash elements does not return an error but rather returns the "null"-equivalent of the hash's value type. In order to find out whether the element actually exists, no special testing operator is used but the hash-access actually evaluates into two values: The indexed value and a boolean value determining whether the first value has really been retrieved from the hash. This is very odd as it easily allows to incorrectly assume a hash value exists, because the second boolean value is never checked or even retrieved.
* Ada does not have any built-in support for hash tables, but there are packages for it in the standard library.
* Tcl has associative arrays which are in fact hashes. Although the contents of an array can be set from and converted to a simple string, the hashes can also be modified using array-specific commands in order to avoid such conversion. Commands for efficient enumeration of all array elements in an unspecified order are available. There is also support for removing all elements of an array, or all elements which match a glob pattern. Unfortunately, for accessing array elements, the lookup key must not contain certain characters (especially unbalanced parentheses) or the interpreter will syntactically mis-interpret the logically correct statement. This even applies if the index is the result of a variable substitution. This means, hash key values in Tcl are restricted and must follow certain rules. It is especially not possible to use arbitrary strings let alone arbitrary binary data strings as hash keys. Instead, hash keys need to be quoted, base-64 encoded or similar in order to ensure reliable operation in such cases.
* Wren uses the same syntax to index hashes (which are called "Map"s) as for indexing "List"s. The difference is the type of the hash key: "Num" (C "double"s) for indexing "Sequence"s, and "String" for indexing "Map"s. Map literals use curly braces and JavaScript/JSON syntax. Different from Lua, there is no syntactic sugar to omit the double quotes when specifying the key in a "Map" literal or as an index.
* Ruby supports hash table objects which can be created by hash table initializer expressions. Hash table initializers are a list of comma-separated "key: value" expressions enclosed in curly braces. The keys used within the initializers can either be quoted strings or unquoted literal identifiers. Hash-accesses look like h[k] where k must have a special type for hash keys and must not be any other type (especially not string or numeric). Ruby's ":" prefix-operator can be used to convert a string or an unquoted literal identifier into a hash key object suitable for indexing a hash table object. Note that the syntax for accessing hashes and and arrays is the same in Ruby - only the type of the index value determines the type of indexing.

Tail calls:
Several programming languages which allow recursive function calls also feature a so-called "tail call optimization". This optimization converts a function call immediately before the current function is exited into sort of a "goto". This makes such tail calls as efficient as a jump statement and therefore allows for arbitrary (and possibly infinite) recursion depth without performance degradation or excessive resource waste (stack space).
* Clojure doesn't currently support TCO. One way to avoid this issue in Clojure is to use the loop and recur special forms. Another way is to use the trampoline  function.
* Scala does not support tail call optimization completely, because the JVM lacks tail call support. In simple cases, the Scala compiler can optimize tail calls into loops.
* Lua provides TCO, but one has to follow a rigid syntactic pattern where to place tail calls within the source code of a function.
* Scheme provides full support for TCO.
* LISP has many variants, and most implementations support TCO. Several implementation, however, do not, and the standard does not enforce it. TCO in LISP is therefore more sort of an extension than a feature one can count on and should therefore be avoided in portable code.
* Ada does not provide any form of guaranteed TCO.
* Tcl adds the "tailcall" command for performing explicit tail calls in version 8.6. This is actually a cleaner solution than the automatic "tail recursion optimization" present in several other languages, because code reviewers can clearly see whether a function call will be a tail call or not.
* Wren automatically optimizes tail calls.

Size of the generated executables:
This is generally only relevant for languages which "compile" source code into some kind of native machine code or "bytecode". The size of executables is normally not be a primary concern these days. But some languages produce exceptionally large executables, which renders them inappropriate for use in embedded or otherwise resource-restricted environments (such as RAM-disk based preboot environments).
* C generates very small executables, at least when dynamically linked. As of 2022 on 32-bit Debian x86, a "hello world" with complete error checking (but using result code instead of error message) was 7.5 kB and 5.6 kB after symbol stripping. It is possible to create even smaller executable by omitting the C standard library, even though the size gains are hardly worth the effort.
* C++ can create executables as small as the ones for C as long as the C++ standard library and certain features which create runtime information metadata within the executable are not used. However, unless special linker options are used, all C++ executables requires C++ shared runtime libraries to be installed in addition to those of C. Those additional libraries have approximately the same size as the C runtime libraries, thus roughly doubling the required disk space for the runtime libraries.
* Go's "hello world" is 3.6 MB (2.3 MB if stripped of debug symbols) statically linked or 18 kB dynamically linked and also stripped (go build -gccgoflags "-s -w").
* Rust's "hello world" generates a whopping 9 MB dynamically-linked executable (2021, on the 32 bit ARM platform) with default options. After stripping of debugging symbols this executable is still 300 kB. However if the option "-C prefer-dynamic=yes" is used, the executable is only 17 kB (14 kB after stripping symbols) but links to an additional runtime library containing the Rust standard library. The latter library is about 5,3 MB large as of 2021, i. e. only half as large as the "hello world" executable which statically linked its contents. Unfortunately, Rust's dynamic runtime library has a very specific name, most likely based on a source checksum or commit ID. Which means that the library is probably neither forward nor backward compatible with different versions of the Rust toolchain. Which means many versions of this library need to be installed unless all executabled have been built with exactly the same toolchain version. This is most likely the reason why "prefer-dynamic=yes" is *not* the default.
* Nim: About 55 kB for a "hello world" after stripping a non-debug build for x86-32. (Dynamically linked; depends only on the C runtime.)

Size of the required runtime dependencies:
Executable binary files, whether they have been created by a compiler directly from the application source code, or rather the interpreter binary for the language or its bytecode-VM itself, usually require other libraries to be available at run time. When packaging applications written in a language for a chroot-evironment, for a virtualized container or for a Linux initramfs environment, all those dependent libraries need to be included in the environment, tool
* C: 2,1 MB for the shared libc, GNU version. Add another 1,1 MB for libm if an application required floating-point support. For a full-fledged application using also dynamic module loading and terminal support, expect about 3,5 MB dependecies total.
* JavaScript: About 62 MB (as of 2020). About half of this size is due to the inclusion of the full-fledged "ICU" UNICODE runtime libraries, and the other half for the node.js shared library. The C and C++ runtime libraries are also included. There are a few additional libraries, but they are only small.
* Julia requires whopping 307 MB of library dependecies before it can evaluate the expression "println(BigInt(2) ^ 128)" passed as a command line switch. Not exactly what your want to depend on in your initramfs.
* Nim: Only the usual C run-time libraries are required for basic applications.

Toolchain:
The language itself is one thing. The other thing is what you need to actually to make it run. This includes shared runtime-libraries and other files which need to be present, such as an interpreter executable. Those aspects are *not* covered here! Instead, this section describes additional tools required only by the developer, but not by the users of a program. This may include things like compilers, linkers, library creation utilities, packaging tools, repository managers, dependency scanners, automated build systems (such as "make"), debuggers, documentation tools etc. Not all possible tools are covered here, only those "typically" being used by a large portion of the developers using the language. Also, only command-line based cross-platform tools are covered that are available for most hardware platforms covered by existing language implementations. This section deliberately ignores the subject of IDEs (Integrated Development Environments), because they are by definition not command line applications. (This also applies to "character mode GUIs" - a full screen editor is more than just a "command line", even if it just runs in a character mode terminal window.)
* C, C++: Those need a C/C++ compiler and linker. Optionally, if static libraries are used, a "librarian" is also required. Larger projects need a build system, where only "make" is generally available virtually everywhere. "imake", "qmake", "cmake" and "scons" are other popular choices for a build system in C projects.
* Rust provides a compiler utility "rustc" and uses the "C" linker. Rust applications consisting of several source files normally use Rust's build and system "cargo". Cargo is more than just a build system, it is also a package manager which can track cross-project dependencies and resolve them by downloading missing dependencies from the Internet. This sounds dangerous to me.
* JAVA ships its own compiler, archiver and other command line tools of part of the JDK (Java Development Kit). The JDK also includes the JRE (Java Runtime Environment), which is required to run any Java applications, including the Java compiler (which is a Java command line application itself). Java projects do not normally use "make" because the startup overhead of any Java application is large. While it is not a problem and in fact normal that a C compiler is invoked once per source file, that would be too slow for the Java compiler. Instead, the Java compiler is usually invoked with many source files as arguments, which it compiles all in a single run of the compiler. Therefore, Java applications usually use Java-specific build systems which invoke the compiler with as many source files as possible. Most Java build systems require XML files with project descriptions as input, and are thus not very human-friendly to edit (they are usually maintained automatically by Java IDEs). Multiple (incompatible) such build systems exist and compete against each other. The most well-known are (in that order) "ant", "maven" and "gradle".

Language and standard library documentation:
This section is *not* about documentation of your own source files or those of third-party developers. It is about the language itself and its standard runtime environment. Some implementations of a programming language ship (at least optionally) with full documentation about one or more of the following aspects: Language reference, standard library reference, tutorials, programming examples. Then this documentation is available for offline-reading (i. e. without Internet access). Other implementations also install local documentation, but require special tools for displaying it, which might not always be easy to get running. The remaining implementations do not ship local documentation at all, they require Internet access for displaying the documentation of even the simplest standard library function.
* JAVA provides full offline documentation of its standard library which can be viewed with any Web-Browser. JavaScript is required to provide the full potential of this help system. The language references for all major language versions can also be downloaded as a PDF (about 5 MB) for offline reading. Of course, everything is available for online browsing, too.
* Go may be shipped with a specialized web-server that can show locally-installed documentation for offline browsing. However, at least on Debian 9, this was not the case. While it is possible to build that document delivery system oneself, it is hard because of version conflicts and the self-hosting nature of the Go toolchain. In general, in order to build the document delivery web-server from source code you need the most current version of the Go compiler installed. In order to build any later version of the Co compiler, you need version 1.4 of it installed first. To get this, you need to build a custom gcc version which supports this Go version. Generally, this is way too complicated and you will not do it. You have to live with the Go online documentation, which however only documents the newest Go version and not the one you might actually have installed on your system.
* Lua is documented in two static HTML files. One of those files is just an index for quickly locating the documentation of a library or C-API function and therefore optional. The remaining file documents everything of importances, that is the language reference, the standard library reference, the C extension API reference, and a less formal introduction into the most important features of the language and its standard interpreter application. In other words, the Lua documentation is awesome!
* AWK (or at least the POSIX version of it) is documented entirely in one single "man"-page. It does not contain examples, but is generally sufficient to master the language and it's built-in runtime-support functions.
* Python ships with full static offline HTML documentation and has in addition a built-in interactive help system which can be used from the command line without access to a web browser. Both help systems document the language as well as the standard library. In addition, the interactive help system can dynamically extract and display the embedded documentation of third-party and locally developed library modules.
* Perl ships with the "perldoc" command-line utility which contains the whole language documentation including all built-in functions and the standard module library. In addition to that, it can also extract and display the embedded documentation of third-party and locally developed library modules. There exists a utility named "podwebserver" which can be installed manually from the CPAN (Perl's online source for installing third-party Perl tools and modules) which runs a specialized web-server for browsing the Perl documentation locally using a web browser. However, installing anything locally using other means than your standard software packaging system is generally a hassle for the user and should be avoided if possible.
* C and C++ generally does not ship with any documentation about the language itself. The developer is expected to get books and documentation on the topic from third parties, which is normally not free. Neither are the official standard documents of the C and C++ standards, which need to purchased also. However, all required information can usually be found online somewhere for free, even though there is no authoritative source for this. Also, the latest draft documents of the standards can usually be downloaded for free, which are typically very similar to the standard documents. At least on POSIX systems, there are generally all functions of the C standard library available as "man"-pages. Additional documentation is therefore only required for C++ and the C language reference. Nowadays, free online documentation is also available, check out wikibooks and [ https://de.cppreference.com/ ].
* Wren's documentation consists of a couple of dozens Markdown files. It is shorter than that for Lua, but the information is spread over more files. The quality of the documentation is generally sufficient, but does not match Lua's.
* Nim ships with excellent documentation (usually as a separate package) in HTML format, originally generated from ReStructured Text markup documentation and extracted source file "documentation comments". It took me a while, however, to find the main document which is named "overview.html". The "index.html" is *not* the main document and has no hyperlinks to the other documents either!

Support for writing correct and portable programs
A correct program must detect failure conditions (especially failed I/O operations) during its execution, and handle them appropriately. The simplest most common way to handle a failure correctly is to terminate a (command-line-) program, returning an exit code indicating failure. Optionally but recommended, an error message should be displayed, too. It must also be possible to report failures detected during cleanup processing, such as during garbage collection or stack unwinding, at least if user-written code (destructors or cleanup handlers) is executed as part of those phases. A very common problem is that many programs which write to standard output in a buffered fashion forget to flush the output buffer before they return, and will therefore never see (let alone handle correctly) any output errors resulting from this.
* C: Yes, with special effort. Provides the necessary means, but does not employ them automatically. puts()/fputs() can be used for writing to standard output and standard error. fflush() flushes unwritten output. All functions indicate success or failure. The constants EXIT_SUCCESS and EXIT_FAILURE represent the success/failure standard exit codes on the local plaform.
* C++: Yes, with special effort. In addition to the same means that are available to C, C++ allows to explicitly enable a stream to throw exceptions on I/O errors using the std::basic_ios::exception() method. Unfortunately, this is not enabled by default.
* Lua: Yes, with special effort. Provides the necessary means when using the "io"-library for output. It does not check any failures using the standard implementation of the built-in "print"-function, however. The os.exit() function can automatically provide platform-specific default values for success/failure exit codes.
* AWK: Yes, out of the box. Does not need any special consideration for writing correct programs. The implementation itself checks for any I/O errors (or other problems) and reports them automatically. There is no exception processing possible or required.
* Python: Yes, with special effort. Output functions generally detect output errors and raise exceptions automatically, but this does not result in a "failure" exit-code by default. However, it is possible to catch all exceptions and return a "failure" exit-code in sich cases explicitly. Perl's POSIX module also provides symbolic constants for the local platform's standard exit codes.
* Go: Yes, with special effort. Similar to C. It is especially annoying that Go (other than C) does provide exception handling via panic/recover, but its standard library does not make any use of it for detecting output errors.
* JAVA: No. JAVA has multiple shortcomings when it comes to implementing a correct "hello"-applications. First, none of the standard output functions throw exceptions on output errors. They notice and remember output errors as part of the stream status, however, and provide checkError() for the application to explicitly check for errors which is quite idiotic. But even worse, at least up to JAVA 8 there is no way to obtain the standard failure exit code of the local platform. On POSIX systems it is possible to launch /bin/false as an external process and remember its exit code. But there is always a chance that launching external processes will not work, and therefore this is not a safe method. Neither is it really portable, because JAVA runs on more platforms than POSIX.
* POSIX shell: Yes, out of the box. While the shell itself does not provide any built-in output commands, it it supposed to use the other available POSIX utilities for this. The "echo"- and "printf"-utilities both detect and report output errors, including an appropriate exit value.
* Embedded usage only. Wren's reference command-line implementation does not seem to check for I/O errors. Even though the fflush() function is called internally, its return value is ignored. The shipped CLI / scripting host is therefore unreliable. But given that Wren is a language for embedding into other languages, it is at least possible to write a better and portable CLI. Another thing to consider is that low-level "release handlers" written in C which are triggered during garbage collection in order to clean up "foreign" objects have no way of reporting failures back to the currently executing language VM instance. They can only terminate the process and do not even have a reliable way of displaying a proper error message. Obviously, such handlers are not eligible for complex cleanup processing such as committing transactions or flushing output buffers; this must already have been done before. They must only close handles and free memory.
* Scala. No. Scala happily writes data to /dev/full without detecting that this does not work. Not even explicitly calling System.out.flush() after writing to standard output changes anything for the better.
* Nim provides the constants QuitSuccess and QuitFailure for portably specifying a return code, and also provided support for flushing out unwritten output buffers. Unfortunately, this function seems to ignore output errors. However, as Nim has full access to the C API, calling fflush(0) explicitly is certainly possible. Therefore Nim allows writing correct programs, just not out of the box.

Licenses:
Most programming languages provide free and open source compilers and/or runtime components as well as commercial implementations. With any compiler or runtime component, it is usually allowed to create open source applications as well as as closed-source application. It is also typically allowed to sell the created applications without paying additional royalties to the compiler/runtime vendor.
* Limbo. Limbo ist a language derived from C with similar design goals as JAVA. It creates byte code to be run by a virtual machine called "dis". The only popular platform with support for "dis" is the Inferno operating system, which is the successor of "Plan 9 from Bell Labs". Unfortunately, INFERNO is only available free of cost as long as all applications written on it are GPL/LGPL licensed. If a closed-source software is to be developed with INFERNO, a commercial license must be purchased which is about  180 plus tax. Considering that the JAVA platform has similar design goals but can be used for free, there is little incentive for using INFERNO.
* In Ada, there are two major versions, one using the viral GPL and the other a modified version of it which allows the use in commerial closed-source applications as well. Normally, the GPL version is a bit more up-to-date with new revisions of the Ada standard. The GPL version is dual-licensed, and a commercial license can be obtained for a lot of money which avoids the viral effect of the GPL. This licensing dilemma (pay or use a a slightly outdated compiler for your commercial applications) might explain why Ada is not as popular as C, although it has many advantages over C (especially regarding the reliability of applications written with it).
* I don't know the precise license terms of Tcl, but as it has been used for years in open as well as commercial products, it must be very permissive and allow almost everything.
* One version of SNOBOL implemented in C has a BSD-like license.
